---
layout: post
title: "[논문리뷰]Speculatvie Decoding"
subtitle: Routing Attention
gh-repo: james-hyeok-kim/james-hyeok-kim.github.io.git
gh-badge: [star, fork, follow]
tags: [AI, LLM, Speculative Decoding]
comments: true
mathjax: true
author: "James Kim"
---

# Speculative Decoding

## Fast Inference from Transformers via Speculative Decoding

저자 : Yaniv Leviathan * 1 Matan Kalman * 1 Yossi Matias 1

출간 : ICML(International Conference on Machine Learning), 2023.

논문 : [PDF](https://arxiv.org/pdf/2211.17192)

---
### Background
* GPT, T5, LaMDA 같은 Autoregressive Transformer 모델은 문장을 생성할 때 한 토큰씩 순차적으로 생성
* 예를 들어 100개의 단어를 생성하려면, 모델을 100번 호출

---

### Introduction

아이디어 요약

1. 작고 빠른 모델 (Mq) 을 먼저 사용해서 여러 개의 다음 토큰을 "추측"합니다.

2. 큰 모델 (Mp) 을 병렬로 실행해 이 추측들이 맞는지 검증합니다.

3. 맞으면 그 추측을 받아들이고, 틀리면 다시 샘플링합니다.

4. 이 과정을 통해 한 번에 최대 𝛾+1개의 토큰을 생성할 수 있습니다.

✅ 핵심: 모델의 출력을 바꾸지 않으면서, 속도만 빠르게 합니다!


(= 원래 Mp 단독으로 생성했을 때와 동일한 분포의 결과를 얻음)

---

### Algorithm

#### 주요 모델 정의
* $𝑀_𝑝$: 원래 느리지만 정확한 대형 모델

* $𝑀_𝑞$ : 빠르고 작은 근사 모델

* $𝛾$ : 한 번에 몇 개의 토큰을 추측할지 결정하는 하이퍼파라미터

#### 알고리즘 흐름
1. $𝑀_𝑞$ 로 $𝛾$ 개의 토큰을 차례로 생성 (예: “나는”, “오늘”, “학교에”)

2. $𝑀_𝑝$ 를 동시에 병렬로 실행해서 해당 토큰들의 확률을 평가함

3. 추측한 토큰들이 $𝑀_𝑝$ 기준으로도 괜찮다면 → 채택

   3-1. 그렇지 않으면 해당 지점부터 다시 샘플링

4. 최종적으로 한 번에 1개에서 최대 $𝛾+1$ 개의 토큰을 생성함


#### 시각적 예시
* (논문 Figure 1 기반): 단계	추측 ($M_q$)	검증 ($M_p$)	결과

1	“나는”, “오늘”, “학교에”	확인함	모두 통과 → 3개 수용

2	“간다”, “그리고”, “밥을”	2번째 틀림 → 1개만 수용	

이런 식으로 병렬성 증가 + 확률 보존을 동시에 만족하는 구조입니다.

---

### Speculatvie Sampling

작은 모델 Mq의 제안을 가지고 큰 모델 Mp가 그 제안을 확률적으로 수용하거나 거절

#### 과정
1. 작은 모델 $𝑀_𝑞$ 에서 $𝑥∼𝑞(𝑥)$ 라는 샘플을 하나 뽑아

2. 이 샘플을 수용할지 말지 결정하기 위해, 큰 모델 $M_p$ 의 확률 분포 $𝑝(𝑥)$와 비교

#### 수용 조건
* $𝑞(𝑥)≤𝑝(𝑥)$ 이면 그냥 수용!

* $𝑞(𝑥)>𝑝(𝑥)$ 이면 확률적으로 거절할 수도 있음:
  * 거절 확률: $1−\frac{𝑝(𝑥)}{𝑞(𝑥)}$ 이 경우, 다시 샘플링을 진행해야 해요.

이 방법은 **Rejection Sampling (거절 샘플링)**의 일반화된 형태이면서, 동시에 효율을 크게 개선한 버전이에요.

#### 정당성
논문 부록 A.1에서는 이렇게 증명

##### 변수 정의
* $𝑝(𝑥)$ : 큰 모델이 주는 실제 분포

* $𝑞(𝑥)$ : 작은 모델이 주는 제안 분포

* $𝑥′$ : 어떤 특정 토큰 (예: "학교에")

* 특정 토큰 $𝑥′$ 이 선택될 확률 $𝑃(𝑥=𝑥′)$

##### 수용된 경우

$$P_{수용}(x=x′)=min(p(x′),q(x′))$$

* $𝑞(𝑥′)<𝑝(𝑥′)$ : 수용 확률 = $𝑞(𝑥′)$
* $𝑞(𝑥′)>𝑝(𝑥′)$ : 수용 확률 = $𝑝(𝑥′)$
* 때문에 $min(p(x′),q(x′))$으로 표현 가능

##### 거절된 경우

$$P_{거절 후 선택}(x=x′)=p(x′)−min(p(x′),q(x′))$$

* 거절 후 다시 샘플링


##### 전체 확률
$$ P(x=x′) = min(p(x′),q(x′)) + p(x′)−min(p(x′),q(x′)) = p(x′)$$
* 수용될 확률 + 거절 후 다시 뽑힐 확률

---

### Speculative Decoding Algorithm 1

```python
Inputs: Mp, Mq, prefix

# 1. 작은 모델로 γ개 토큰 추측
for i = 1 to γ:
    xi ~ Mq(prefix + x1 + ... + xi-1)

# 2. 큰 모델로 prefix들에 대해 동시에 평가
p1, ..., pγ+1 ~ Mp(prefix), Mp(prefix+x1), ..., Mp(prefix+x1+...+xγ)

# 3. 수용 여부 판단
n = 가장 먼저 수용 실패하는 위치

# 4. 다시 샘플링 (보정)
if n < γ:
    p′(x) = norm(max(0, pn+1(x) - qn+1(x)))
else:
    p′(x) = pn+1(x)

# 5. 최종 토큰 t ~ p′(x)
return prefix + [x1, ..., xn, t]
```

---

### 성능 분석
1. 한 번의 실행에서 평균적으로 몇 개의 토큰을 생성할 수 있는가?

2. 전체 디코딩 시간(walltime)이 얼마나 줄어드는가?

3. 전체 연산량(arithmetic operations)은 얼마나 증가 또는 감소하는가?



#### 3.1. 생성되는 토큰 수 (E[# tokens])
#### 정의: 수용률 𝛽

* 특정 prefix $𝑥<𝑡$에 대해, 작은 모델 $𝑀_𝑞$ 의 예측을 큰 모델 $𝑀_𝑝$이 수용할 확률

$$ 𝛽=𝐸_{𝑥∼𝑞(𝑥)}[min⁡(1,\frac{𝑝(𝑥)}{𝑞(𝑥)})]=\displaystyle\sum_x min⁡(𝑝(𝑥),𝑞(𝑥)) $$

* 이 값이 높을수록 작은 모델이 큰 모델을 더 잘 근사한다는 뜻

* 평균 수용률을 𝛼 =𝐸[𝛽]라고 정의

#### 기대 생성 토큰 수 (γ+1개까지 가능)
* 한 번의 Algorithm 1 실행에서 평균적으로 생성되는 토큰 수는 다음과 같습니다:

$$𝐸[tokens] = \frac{1−𝛼^{𝛾+1}}{1−𝛼}$$

* 예시
$𝛼=0.8, 𝛾=3$
$$𝐸[tokens]=\frac{1−0.8^{4}}{1−0.8} = \frac{1−0.4096}{0.2}=2.95$$

즉, 평균적으로 거의 3개 생성 (원래는 1개씩만 가능했음)

* 𝛾개 토근 제안하고, 하나라도 틀리면 멈추기 때문에 평균 수용률  $\alpha$는 아래 공식으로 유도
  
* 수용된 토큰 수 𝑛은 다음을 따릅니다:

$$𝑃(𝑛=𝑘)=𝛼^𝑘(1−𝛼), \ \ \ for \ 𝑘<𝛾$$
$$𝑃(𝑛=𝛾)=𝛼^{𝛾} $$
 
* 즉,γ번 연속 수용되면 멈추지 않고, 그 다음 토큰도 하나 더 만듭니다.
→ n+1, 즉 최대 𝛾+1


그래서 기대값은 

$$𝐸[tokens]=\displaystyle\sum_{𝑘=0}^{𝛾−1}(𝑘+1)𝛼^𝑘(1−𝛼)+(𝛾+1)𝛼^𝛾$$

(등비수열의 합공식)

$$E[tokens]= \frac{1−α^{γ+1}}{1−α}$$



#### 3.2. Walltime 개선
* 변수 정의

c: 작은 모델 한 번 실행 시간 / 큰 모델 한 번 실행 시간 비율

(예: T5-small은 T5-XXL보다 훨씬 빠르니 𝑐≪1)

#### 기대 속도 개선 비율:

$$Improvement = \frac{1−α^{γ+1}}{(1−α)(γc+1)}$$

* α: 수용률 (작은 모델의 제안이 큰 모델에 의해 받아들여질 확률)

* γ: 추측하는 토큰 수

* c: 작은 모델(Mq)의 실행 시간 / 큰 모델(Mp)의 실행 시간 비율

* γc: 작은 모델을 γ번 실행하는 데 드는 시간

* +1: 큰 모델을 한 번 실행하는 데 드는 시간



 
##### 예시
* α=0.75, γ=7, c=0.02

$$ Improvement ≈ \frac{1−0.75^{8}}{(1−0.75)(0.14+1)} ≈ \frac{0.9}{0.25 \odot 1.14} ≈ 3.16x $$


### 3.3. 전체 연산량 (Arithmetic Operations)
병렬성은 증가했지만, 계산량은 줄어들 수도 있고, 오히려 늘어날 수도 있습니다.

#### 기대 연산량 증가 비율:

$$Ops \ Increase = \frac{(1−α)(γ \hat{c}+γ+1)}{1-α^{γ+1}}$$

(한 토큰당 연산량 with Algorithm 1)÷(기존 Mp에서 한 토큰당 연산량)

​
$\hat{c}$ : 작은 모델의 연산량 / 큰 모델의 연산량 비율

→ 이 값이 1보다 크면 연산량은 늘어나지만, walltime은 줄어듦

→ 실전에서는 메모리 대역폭이 병목이기 때문에 이 손해는 감수할 만함

### 3.4. γ 값 선택 (성능 vs 비용 최적화)
* γ가 너무 작으면 병렬 효과가 적고,

* γ가 너무 크면 연산 낭비가 많음

논문에서는 γ에 따라 벽시간 개선량이 볼록 함수 형태로 존재함을 수치적으로 보여줍니다.


# vLLM
(Versatile Large Language Model)

vLLM은 대규모 언어 모델의 배포를 효율적이고 확장 가능하게 만드는 추론 라이브러리입니다. 

특히 메모리 사용을 최적화하고 높은 처리량을 달성하는 데 중점을 둡니다.

## 주요 특징

- **PagedAttention:** 운영체제의 가상 메모리 관리 방식에서 영감을 받아, KV 캐시를 비연속적인 메모리 블록에 저장하는 PagedAttention 기법을 도입했습니다. 이를 통해 메모리 단편화를 줄이고 더 긴 컨텍스트 윈도우를 지원하여 메모리 효율성을 극대화합니다.
- **Continuous Batching (연속 배치 처리):** 들어오는 요청을 실시간으로 동적으로 배치하여 처리합니다. 유사한 쿼리의 계산을 개별적으로 처리하는 대신 배치 처리하여 처리량을 개선하고 메모리 할당을 최적화합니다. 이는 GPU 자원 활용률을 높여 최대 23배의 처리량 향상과 지연 시간 감소를 달성할 수 있습니다.
- **최적화된 CUDA 커널 및 FlashAttention 통합:** GPU 성능을 극대화하기 위해 FlashAttention 및 FlashInfer와 같은 최적화된 CUDA 커널을 통합하여 트랜스포머 모델의 효율성을 개선합니다.
- **다양한 양자화 지원:** 모델의 크기를 줄이고 추론 속도를 높이기 위해 GPTQ, AWQ, INT4, INT8, FP8 등 다양한 양자화 방식을 지원합니다. 이를 통해 정확도를 유지하면서도 모델을 경량화할 수 있습니다.
- **비동기 엔진 구조:** 추론 요청을 효율적으로 처리하기 위한 비동기 처리 구조를 통해 높은 처리량을 유지합니다.
- **OpenAI 호환 API 지원:** OpenAI API와 동일한 언어로 작동하는 고성능 백엔드 서버로 사용될 수 있어, 기존 OpenAI를 위해 설계된 애플리케이션에 쉽게 통합할 수 있습니다.
- **광범위한 모델 지원:** Llama, Mistral & Mixtral 등 Hugging Face Hub에서 호스팅되는 다양한 인기 변환기 기반 모델을 지원합니다.

PagedAttention은 KV 캐시

# **몇 가지 주요 vLLM 용어:**

vLLM은 겉으로 보기에는 사용하기 쉽지만, 몇 가지 핵심 개념을 이해하면 *그 이유*를 감상하는 데 도움이 됩니다:

- **PagedAttention:** 이것은 vLLM의 주력 기능입니다. 전통적인 주의 메커니즘에서는 키-값(KV) 캐시(생성을 위한 중간 결과를 저장하는)가 연속적인 메모리 블록을 요구합니다. 이로 인해 단편화와 메모리 낭비(내부 및 외부)가 발생합니다. PagedAttention은 운영 체제의 가상 메모리처럼 작동합니다. KV 캐시를 비연속 블록(페이지)으로 나누어 훨씬 더 유연하고 효율적인 메모리 관리를 가능하게 합니다. 이는 메모리 오버헤드를 크게 줄이며(개발자가 보고한 경우 최대 90%까지) 메모리 중복 없이 공유 접두사와 같은 기능을 가능하게 합니다.
- **지속적인 배치 처리:** 고정된 수의 요청이 도착할 때까지 계산을 기다리는 대신(정적 배치), 지속적인 배치 처리는 vLLM 엔진이 배치 내에서 이전의 시퀀스 생성이 완료되는 즉시 새 시퀀스를 처리하기 시작할 수 있도록 합니다. 이를 통해 GPU가 지속적으로 바쁘게 유지되어 처리량을 극대화하고 요청의 평균 대기 시간을 줄입니다.

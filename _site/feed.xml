<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    
    <title>AI 리부팅 (AI Rebooting)</title>
    
    
    <description>This website is a virtual proof that I&apos;m awesome</description>
    
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
    
      <item>
        <title>[논문리뷰]Denoising Diffusion Implicit Models(DDIM)</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          DDIM (Diffusion) - 
          Denoising Diffusion Implicit Models 저자 : Jiaming Song, Chenlin Meng, Stefano Ermon 논문 : PDF 일자 : Submitted on 6 Oct 2020 (CVPR, Computer Vision and Pattern Recognition) Published as a conference paper at ICLR 2021 Summary DDIM (Denoising Diffusion Implicit Models)은 Denoising Diffusion Probabilistic Models (DDPMs)의 샘플링 속도를 개선한...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Denoising-Diffusion-Implicit-Models(DDIM)/</link>
        <guid isPermaLink="true">/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Denoising-Diffusion-Implicit-Models(DDIM)/</guid>
      </item>
    
      <item>
        <title>[논문리뷰]Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015 ICML)</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          Diffusion - 
          Deep Unsupervised Learning using Nonequilibrium Thermodynamics 저자(소속) : Jascha Sohl-Dickstein (Stanford University) Eric A. Weiss(University of California, Berkeley) Niru Maheswaranathan (Stanford University) Surya Ganguli(Stanford University) 논문 : PDF 일자 : 18 Nov 2015, ICML(International conference on machine learning) 1. Introduction 1.1 문제 설정: Tractability vs Flexibility 배경 문제: 머신러닝에서는 복잡한...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Deep-Unsupervised-Learning-using-Nonequilibrium-Thermodynamics-(2015-ICML)/</link>
        <guid isPermaLink="true">/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Deep-Unsupervised-Learning-using-Nonequilibrium-Thermodynamics-(2015-ICML)/</guid>
      </item>
    
      <item>
        <title>Flash Attention</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI Flash Attention - 
          


        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-Flash-Attention/</link>
        <guid isPermaLink="true">/2025-08-21-Flash-Attention/</guid>
      </item>
    
      <item>
        <title>vLLM</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          vLLM (Versatile Large Language Model) vLLM은 대규모 언어 모델의 배포를 효율적이고 확장 가능하게 만드는 추론 라이브러리입니다. 특히 메모리 사용을 최적화하고 높은 처리량을 달성하는 데 중점을 둡니다. 주요 특징 PagedAttention: 운영체제의 가상 메모리 관리 방식에서 영감을 받아, KV 캐시를 비연속적인 메모리 블록에 저장하는 PagedAttention 기법을 도입했습니다. 이를 통해 메모리 단편화를 줄이고...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-vLLM/</link>
        <guid isPermaLink="true">/2025-08-21-vLLM/</guid>
      </item>
    
      <item>
        <title>Tensor_Parallelism.</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          분산 처리 3 - Pipeline Parallelism과 Tensor Parallelism에 관하여 Tensor Parallelism Tensor Parallelis (TP)에 관해 살펴보자. TP는 쉽게 말해 모델 자체 (전체 파라미터)를 여러 개의 부분으로 쪼개어 서로 다른 GPU에 올리는 방식이라고 볼 수 있다. Layer별로 나누어 GPU에 올리는 PP와는 완전히 다른 방식이다. PP는 세로로 자르는 방식이라면 TP는 가로로 자르는...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-Tensor_Parallelism/</link>
        <guid isPermaLink="true">/2025-08-21-Tensor_Parallelism/</guid>
      </item>
    
      <item>
        <title>Speculative Decoding &amp; Tensor Parallelism</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          Speculative Decoding 1. 모델 사이즈 고정 (3B 파라미터) → Hidden Dimension vs Sequence Length Trade-off Sequence Length $\leftrightarrow$ Hidden Dimension은 Trade off관계 Hidden dim이 크면: 연산량이 증가하지만 한 번에 처리하는 token 수(=sequence length)는 메모리 상 한계로 줄어듦. Hidden dim이 작으면: 연산량은 줄지만 sequence length를 길게 가져갈 수 있음 (batching 효과)....
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-Speculative-Decoding-&-Tensor-Parallelism/</link>
        <guid isPermaLink="true">/2025-08-21-Speculative-Decoding-&-Tensor-Parallelism/</guid>
      </item>
    
      <item>
        <title>RoPE</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI-Rotary Position Embedding - 
          https://g3lu.tistory.com/38 https://mari970.tistory.com/49 RoPE PDF Rotary Position Embedding Transformer 모델은 본질적으로 순서에 무관, Position embedding통해 위치 정보를 보존 회전행렬(Rotation Matrix)를 통해 고유각도 부여 Absolute Position 인코딩 가능 Relative Position의 Dependency를 통합 복소 평면상에서 회전을 토큰위치정보를 통해 벡터에 표현. Query m Key n Formulation Position-Encoded Query and Key Vectors \(f_q(x_m,m) = (W_qx_m)e^{im\theta}\)...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-RoPE/</link>
        <guid isPermaLink="true">/2025-08-21-RoPE/</guid>
      </item>
    
      <item>
        <title>RAG</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          RAG RAG는 Retrieval-Augmented Generation의 줄임말로, 외부 지식을 검색(Retrieve)해서 LLM의 생성(Generation)에 보조 정보로 추가하는 방식입니다. RAG는 LLM이 학습하지 않은 정보에 대해 답변을 잘 못하는 문제를 해결하기 위해, 외부 데이터 소스를 활용하여 답변의 정확도를 높이는 기술 왜 필요한가? 기존 LLM (GPT, Claude, PaLM 등)은 고정된 파라미터 안에만 지식을 담고 있어서: 최신 정보...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-RAG(Retrieval-Augmented-Generation)/</link>
        <guid isPermaLink="true">/2025-08-21-RAG(Retrieval-Augmented-Generation)/</guid>
      </item>
    
      <item>
        <title>Paged Attention</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          Paged Attention KV cache의 특징으로는 Large: 단적으로 LLaMA-13B를 예로 들었을 때 단일 시퀀스에 대해 최대 1.7GB를 차지함 Dynamic: 시퀀스 길이에 따라 크기가 달라지며, 가변적이고 예측이 힘듬. 기존 시스템은 fragmentation과 over-reservation으로 인해 60~80%의 메모리를 낭비 PagedAttention에서 메모리 낭비는 시퀀스의 마지막 블록에서만 발생한다. 그리고 실제 메모리 낭비는 4% 미만으로 거의 최적에 가까운...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-Paged_Attention/</link>
        <guid isPermaLink="true">/2025-08-21-Paged_Attention/</guid>
      </item>
    
      <item>
        <title>Normalization Techniques </title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          Normalization Techniques Batch &amp;amp; Channel &amp;amp; Layer Normalization \[y = \frac{(x - mean)}{\sqrt{(var + eps)}}\] RMS Normalization Root Mean Square Layer Normalization \[y=\frac{x}{\sqrt{(mean(x²)+ε)}}\] 구분 Layer Normalization RMSNorm 정규화 방식 평균과 분산을 사용하여 정규화 제곱평균을 사용하여 정규화 계산 비용 평균과 분산 계산,비용이 더 높음 제곱평균 계산만, 비용이 더 낮음 파라미터 𝛾...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-Normalization_Techniques/</link>
        <guid isPermaLink="true">/2025-08-21-Normalization_Techniques/</guid>
      </item>
    
      <item>
        <title>LangChain</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          LangChain 정의 LangChain은 대규모 언어 모델(LLM)을 실제 애플리케이션에 쉽게 통합하고 구성할 수 있도록 도와주는 Python 기반 오픈소스 프레임워크 기존의 언어 모델이 주로 텍스트 생성에 중점을 둔 반면, LangChain은 다양한 외부 데이터 소스와 통합하여 보다 복잡하고 유용한 애플리케이션을 만들 수 있도록 설계 Chain을 사용하면 여러 구성 요소를 결합하여 하나의 종합적인 애플리케이션을...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-LangChain/</link>
        <guid isPermaLink="true">/2025-08-21-LangChain/</guid>
      </item>
    
      <item>
        <title>LLM Evaluation.</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          LLM 평가 OpenAI Eval 라이브러리 HellaSwag, TrughfulQA, MMLU Open-Ko-LLM 리더보드 ARC, HellaSwag, MMLU, TruthfulQA, Text Generation 1. Coding Task 1-1. HumanEval : LLM Benchmark for Code Generation OpenAI가 공개한 HumanEval Data Set에는 signature, docstring, body, several unit test들이 포함된 164개의 프로그래밍 문제가 포함되어 있습니다. 코드 생성 모델의 traning set에 포함되지...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-LLM-Evaluation/</link>
        <guid isPermaLink="true">/2025-08-21-LLM-Evaluation/</guid>
      </item>
    
      <item>
        <title>KV Cache</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          KV Cache KV caching 을 사용하면 행렬곱셈은 훨신 빨라지지만 행렬 state 를 저장하기 때문에 많은 GPU VRAM 이 필요하다는 단점이 있다. 또한 context length 와 batch size 가 증가할수록 KV cache 요구량이 매우 증가한다. kv cache 의 특징 Large : 1개 sequence 에 대해 LLaMA-13B 모델에서 최대 1.7GB 를 잡아먹는다....
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-KV_Cache/</link>
        <guid isPermaLink="true">/2025-08-21-KV_Cache/</guid>
      </item>
    
      <item>
        <title>In-Flight Batching</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          In-Flight Batching Adaptive batching의 경우, 아래 이미지와 같이 batch 하나에 대해서 longest 기준에 맞춰 padding을 해서 inference를 하게 됩니다. Transformer의 encoder 기반 모델들은 한 번에 추론을 하기 때문에 저런 방식이 적합하지만, decoder 기반의 생성형 모델들의 경우에는 하나씩 생성하기 때문에 위의 방식이 비효율적이게 됩니다. 3개를 생성하면 input2에 대해서는 더 이상 생성할...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-In-Flight_Batching/</link>
        <guid isPermaLink="true">/2025-08-21-In-Flight_Batching/</guid>
      </item>
    
      <item>
        <title>FiLM</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          FiLM (Feature-wise Linear Modulation) Paper 신경망 중간 특징을 조건정보를 기반으로 선형 변환(Affine Transformation) 신경망 계산의 조건화 방법 (Conditioning) \(FiLM(h, \gamma, \beta) = \gamma \odot h + \beta\) Terminology $\odot$ = element wise multiplication $\gamma$ = scaling factor $\beta$ = shifting factor 장점 시각적 추론 : 이미지관련 태스크에서 질문에 따른 변조...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-FiLM/</link>
        <guid isPermaLink="true">/2025-08-21-FiLM/</guid>
      </item>
    
      <item>
        <title>F1 Score</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          F1 Score 분류 모델의 precision 과 Recall성능을 동시에 고려 Precision과 Recall의 조화평균, 0~1사이 Accuracy (정확도) Precision (정밀도) Recall (재현율) Precision-Recall Curve Precision과 Recall은 Trade-off관계 Decision Threshod를 통해 조절 AP (Average Precision) Precision-Recall Curve에서 그래프 아래쪽 면적 TNR (True Negative Rate) 실제 Negative 샘플 중 분류 모델이 Negative로 판정한 비율 FPR...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-F1-Score/</link>
        <guid isPermaLink="true">/2025-08-21-F1-Score/</guid>
      </item>
    
      <item>
        <title>Continuous Batching</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          Continuous Batching Batch ML에서 Batch는 일반적으로 훈련 데이터를 나누는 단위 그러나, Inference 단계에서도 Batch라는 용어가 사용 모델이 동시에 처리 하는 입력 데이터의 묶음을 나타냄 여러 입력에 대한 예측을 동시에 수행 LLM인퍼런스에서 Batching의 중요성 질문이 프롬프트에 들어오면 prefil phase에서 전체 attetnion matrix 계산 이후 답변을 위한 토큰 생성을 decode라고 하고 이때는...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-Continuous_Batching/</link>
        <guid isPermaLink="true">/2025-08-21-Continuous_Batching/</guid>
      </item>
    
      <item>
        <title>Attention_mechanism</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI - 
          Reference Attention Mechanism Attention 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(step)마다, 인코더의 입력 시퀀스를 다시 참고 어텐션 함수는 주어진 ‘쿼리(Query)’에 대해 모든 ‘키(Key)’의 유사도 이 유사도를 키(Key)와 매핑되어 있는 각각의 ‘값(Value)’에 반영 ‘유사도가 반영된’ 값(Value)을 모두 더해서 리턴하고, 어텐션 값(Attention value)를 반환 Dot-Product Attention Decoder의 세번째 LSTM Cell에서...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-Attention_mechanism/</link>
        <guid isPermaLink="true">/2025-08-21-Attention_mechanism/</guid>
      </item>
    
      <item>
        <title>Activation function (Non-linear)</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          AI-Activation - 
          Activation function (Non-linear) GELU Gaussian Error Linear Unit \(GELU(x) = xP(X \leq x) = x\sigma(x) = x \cdot \frac{1}{2} \[1+erf(x/\sqrt{2})\]\) \(or\) \(GELU(x) = 0.5x(1+tanh[\sqrt{2/\pi}(x+0.044715x^3)])\) \(or\) \(GELU(x) = x \sigma(1.702x)\) ERF (Error Function) ERF 함수는 값으로 구성된 간격에 대한 가우스 오차 함수의 적분값을 반환합니다. $ erf(x) &amp;lt; 1$ \[erf(x) = \frac{2}{\sqrt{\pi}}...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-Activation_function/</link>
        <guid isPermaLink="true">/2025-08-21-Activation_function/</guid>
      </item>
    
      <item>
        <title>[논문리뷰]Fast Inference from Transformers via Speculative Decoding</title>
        
        <dc:creator><![CDATA[ James Kim ]]></dc:creator>
        
        <description>
          Speculative Decoding - 
          Speculative Decoding Fast Inference from Transformers via Speculative Decoding 저자 : Yaniv Leviathan * 1 Matan Kalman * 1 Yossi Matias 1 출간 : ICML(International Conference on Machine Learning), 2023. 논문 : PDF Background GPT, T5, LaMDA 같은 Autoregressive Transformer 모델은 문장을 생성할 때 한 토큰씩 순차적으로 생성 예를 들어...
        </description>
        <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
        <link>/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Fast-Inference-from-Transformers-via-Speculative-Decoding/</link>
        <guid isPermaLink="true">/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Fast-Inference-from-Transformers-via-Speculative-Decoding/</guid>
      </item>
    
  </channel>
</rss>

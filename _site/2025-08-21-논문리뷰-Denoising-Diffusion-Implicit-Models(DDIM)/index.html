<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 6.0.1 | Copyright Dean Attali 2023 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  

  <title>[논문리뷰]Denoising Diffusion Implicit Models(DDIM) | AI 리부팅 (AI Rebooting)</title>

  
  
  <meta name="author" content="James Kim">
  

  <meta name="description" content="DDIM (Diffusion)">

  

  

  
  <link rel="alternate" type="application/rss+xml" title="AI 리부팅 (AI Rebooting)" href="/feed.xml">
  

  

  

  

  

  
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script"></script>



  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="AI 리부팅 (AI Rebooting)">
  <meta property="og:title" content="[논문리뷰]Denoising Diffusion Implicit Models(DDIM) | AI 리부팅 (AI Rebooting)">
  <meta property="og:description" content="DDIM (Diffusion)">

  
  <meta property="og:image" content="/assets/img/AI.jpg">
  

  
  <meta property="og:type" content="article">
  
  <meta property="og:article:author" content="James Kim">
  
  <meta property="og:article:published_time" content="2025-08-21T00:00:00-04:00">
  <meta property="og:url" content="/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Denoising-Diffusion-Implicit-Models(DDIM)/">
  <link rel="canonical" href="/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Denoising-Diffusion-Implicit-Models(DDIM)/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="[논문리뷰]Denoising Diffusion Implicit Models(DDIM) | AI 리부팅 (AI Rebooting)">
  <meta property="twitter:description" content="DDIM (Diffusion)">

  
  <meta name="twitter:image" content="/assets/img/AI.jpg">
  

  


  

  
  

  

</head>


<body>
  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="/">AI 리부팅 (AI Rebooting)</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/about%20me">About Me</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://www.linkedin.com/in/jameskim525">Author's home</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="/">
          <img alt="Navigation bar avatar" class="avatar-img" src="/assets/img/AI.jpg" />
        </a>
      </div>
    </div>
  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: '/assets/data/searchcorpus.json' 
    });
  </script>
</div>





  



<header class="header-section ">
<div class="intro-header ">
  
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>[논문리뷰]Denoising Diffusion Implicit Models(DDIM)</h1>
          
            
              <h2 class="post-subheading">DDIM (Diffusion)</h2>
            
          
          
           
            
              By <strong>James Kim</strong><br>
            
            <span class="post-meta">Posted on August 21, 2025</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
  
  
</div>



</header>


<main class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      
        
        
        

        <div id="header-gh-btns">
          
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=james-hyeok-kim&repo=james-hyeok-kim.github.io.git&type=star&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=james-hyeok-kim&repo=james-hyeok-kim.github.io.git&type=fork&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=james-hyeok-kim&type=follow&count=true" frameborder="0" scrolling="0" width="220px" height="20px"></iframe>
              
            
          
        </div>
      

      

      <div class="blog-post">
        <h1 id="denoising-diffusion-implicit-models">Denoising Diffusion Implicit Models</h1>

<p>저자 : Jiaming Song, Chenlin Meng, Stefano Ermon</p>

<p>논문 : <a href="https://arxiv.org/pdf/2010.02502">PDF</a></p>

<p>일자 : Submitted on 6 Oct 2020  (CVPR, Computer Vision and Pattern Recognition)</p>

<p>Published as a conference paper at ICLR 2021</p>

<h2 id="summary">Summary</h2>

<ul>
  <li>DDIM (Denoising Diffusion Implicit Models)은 Denoising Diffusion Probabilistic Models (DDPMs)의 샘플링 속도를 개선한 모델</li>
  <li>DDIM은 DDPM과 동일 훈련 방식을 사용</li>
  <li>DDIM은 비마르코프(non-Markovian) 사용, 샘플링에 필요한 단계 감소</li>
</ul>

<p align="center">
<img width="653" height="139" alt="image" src="https://github.com/user-attachments/assets/bd4fc49f-f068-4d6c-b6d6-316d3b6c5a31" />
</p>

<h3 id="ddpm-한계">DDPM 한계</h3>
<ul>
  <li>
    <p>DDPM은 샘플 하나를 만들기 위해 수많은 마르코프 체인(Markov chain) 시뮬레이션</p>
  </li>
  <li>
    <p>예를 들어, Nvidia 2080 Ti GPU를 기준으로 32x32 크기 이미지 5만 개를 생성하는 데 DDPM은 약 20시간이 걸리지만, GAN은 1분도 채 걸리지 않습니다.</p>
  </li>
</ul>

<h3 id="ddim의-핵심-아이디어-the-core-idea-of-ddim">DDIM의 핵심 아이디어 (The Core Idea of DDIM)</h3>

<ul>
  <li>
    <p>훈련과 샘플링의 분리 (Decoupling Training and Sampling): DDIM의 핵심은 훈련에 사용되는 목표 함수가 마르코프 확산 과정뿐만 아니라 다양한 비마르코프 확산 과정에도 동일하게 적용될 수 있다는 점</p>
  </li>
  <li>
    <p>DDPM으로 이미 학습된 모델을 그대로 DDIM의 생성 과정을 구현할 수 있으며, 추가적인 재훈련이 필요 없습니다.</p>
  </li>
  <li>
    <p>가속화된 샘플링 (Accelerated Sampling): DDIM은 생성 과정을 짧은 단계(short generative Markov chains)로 시뮬레이션할 수 있도록 설계되었습니다.</p>
  </li>
  <li>
    <p>DDPM보다 10~50배 더 빠르게</p>
  </li>
  <li>
    <p>샘플링에 필요한 단계 수(S)를 조절함으로써 계산량과 샘플 품질 사이의 균형을 맞출 수 있습니다.</p>
  </li>
</ul>

<h3 id="ddim의-주요-특징-및-장점-key-features-and-benefits-of-ddim">DDIM의 주요 특징 및 장점 (Key Features and Benefits of DDIM)</h3>
<ol>
  <li>샘플 품질 및 효율성 (Sample Quality and Efficiency):</li>
</ol>

<ul>
  <li>DDIM은 DDPM보다 적은 샘플링 단계(S)에서 더 우수한 샘플 품질</li>
</ul>

<ol>
  <li>샘플 일관성 (Sample Consistency):</li>
</ol>

<ul>
  <li>DDIM의 생성 과정은 결정론적(deterministic)이므로, 동일한 초기 잠재 변수($x_T$)에서 이미지의 고수준 특징(high-level features)이 유사하게 유지됩니다.</li>
</ul>

<ol>
  <li>의미 있는 이미지 보간 (Semantically Meaningful Image Interpolation):</li>
</ol>

<ul>
  <li>
    <p>DDIM의 일관성 덕분에, 잠재 공간($x_T$ 공간)에서 직접 보간(interpolation)을 수행하여 의미 있는 이미지 변환을 만들어낼 수 있습니다.</p>
  </li>
  <li>
    <p>GAN과 유사한 이 특성은 잠재 변수를 조작하여 생성되는 이미지의 고수준 특징을 직접 제어할 수 있게 해줍니다. DDPM은 확률적(stochastic) 생성 과정 때문에 이러한 보간이 어렵습니다.</p>
  </li>
</ul>

<ol>
  <li>잠재 공간으로부터의 재구성 (Reconstruction from Latent Space):</li>
</ol>

<ul>
  <li>
    <p>DDIM은 이미지($x_0$)를 잠재 변수($x_T$)로 인코딩한 후, 다시 $x_0$로 재구성하는 작업에 활용될 수 있습니다.</p>
  </li>
  <li>
    <p>DDIM은 DDPM보다 낮은 재구성 오류를 보이며, 이는 DDPM의 확률적 특성 때문에 불가능한 기능입니다.</p>
  </li>
</ul>

<h3 id="1-서론introduction">1. 서론(Introduction):</h3>

<p>DDPM의 배경과 장점(적대적 학습 불필요)을 소개하고, 느린 샘플링이라는 치명적인 단점을 다시 강조하며 DDIM을 제안하는 동기를 설명합니다.</p>

<h3 id="2-배경background">2. 배경(Background):</h3>

<p>DDPM의 순방향 확산 과정과 역방향 생성 과정에 대한 수학적 정의를 설명합니다.</p>

<h4 id="forward-process-diffusion-process-q">Forward Process (Diffusion Process) $q$</h4>

<ul>
  <li>$q(x_{1:T}|x_0) := \prod^T_{t=1}q(x_t|x_{t−1}),\ where \ q(xt|xt−1) := \mathcal{N} \left( \sqrt{\frac{\alpha_t}{\alpha_{t-1}}x_{t-1}}, \left(1 - \frac{α_t}{α_{t−1}} \right)I \right) \ (3)$</li>
</ul>

<h4 id="reverse-process-p_theta">Reverse Process $p_{\theta}$</h4>
<ul>
  <li>$q(x_t|x_0) := \int q(x_{1:t}|x_0)dx_{1:(t−1)} = \mathcal{N} (x_t;\sqrt{α_t}x_0,(1 − α_t)I)$</li>
  <li>$x_t =\sqrt{α_t}x_0 + \sqrt{1 − α_t}\epsilon, \ where \ \epsilon \sim \mathcal{N} (0, I) \ (4)$</li>
</ul>

\[\begin{align}
p_{\theta}(x_{t-1}\|x_t) = 
\begin{cases} 
N(f_{\theta}^{(1)}(x_1), \sigma_1^2I) &amp; \text{if } t=1 \\
q_{\sigma}(x_{t-1}\|x_t, f_{\theta}^{(t)}(x_t)) &amp; \text{otherwise}
\end{cases}
\end{align}\]

<h4 id="loss">Loss</h4>

<ul>
  <li>
    <p>$L_γ(\epsilon_θ) := \sum^T_{t=1}γ_t \mathcal{E}_{x0∼q(x_0),\epsilon_t \sim \mathcal{N}(0,I)} [\parallel \epsilon^{(t)}_θ(\sqrt{α_t}x_0 + \sqrt{1 − α_t} \epsilon_t) − \epsilon_t \parallel^2_2] \ (5)$</p>
  </li>
  <li>DDPM $\gamma = \frac{β_t^2}{2σ_t^2α_t(1−\bar{α}_t)}$</li>
  <li>$γ = 1$도 가능함을 알게됨(다른논문에서)</li>
  <li>DDIM에서는  $\gamma = 1$을 사용</li>
</ul>

<h4 id="ddpm-vs-ddim">DDPM vs DDIM</h4>
<ul>
  <li>$DDPM \ \bar{\alpha_t} = DDIM \ \alpha_t$</li>
</ul>

<h3 id="ddim-핵심-아이디어-non-markovian-process">DDIM 핵심 아이디어 Non Markovian Process</h3>

<h4 id="새로운-forward-조건부-분포-q">새로운 Forward 조건부 분포 $q$</h4>

<p><a href="https://www.youtube.com/watch?v=n2P6EMbN0pc">Youtube</a></p>

<ul>
  <li>
    <p>$x_t =\sqrt{α_t}x_0 + \sqrt{1 − α_t}\epsilon, \ where \ \epsilon \sim \mathcal{N} (0, I) \ (4)$</p>
  </li>
  <li>
    <p>$q_\sigma(x_t|x_0) := \mathcal{N}(\sqrt{α_t}x_0,(1 − α_t)I)$ 의 분포를 따를 때,</p>
  </li>
</ul>

<p>(4)를 바탕으로 $x_{t-1}$ 예측하기</p>

\[q_σ(x_{t−1}∣x_t,x_0)=\mathcal{N}(\sqrt{α_{t−1}}x_0 +  \sqrt{1−α_{t−1}−σ_t^2} \cdot \frac{x_t− \sqrt{α_t} x_0}{\sqrt{1−α_t}},σ_t^2I) \\ (7)\]

<ul>
  <li>$\sigma_t$ : 확률을 조절하는 새로운 파라미터</li>
</ul>

<h5 id="7-유도과정">(7) 유도과정</h5>

<ul>
  <li>$q_\sigma(x_t|x_0) = \mathcal{N}(\sqrt{α_t}x_0,(1 − α_t)I)$</li>
  <li>$q_\sigma(x_{t-1}|x_0) = \mathcal{N}(\sqrt{\alpha_{t-1}}x_0, (1-\alpha_{t-1}I)$
    <ul>
      <li>$p(x) = \mathcal{N}(x|\mu,\Lambda^{-1})$</li>
      <li>$\Lambda : Lambda$</li>
    </ul>
  </li>
  <li>$p(y|x) = \mathcal{N}(y|Ax + b, L^{-1})$
    <ul>
      <li>y가 x에 대한 선형 변환에 가우시안 노이즈가 더해진 형태 $y=Ax+b+\epsilon$</li>
      <li>이때 노이즈 $\epsilon$은 평균 0, 공분산 $L^{-1}$인 가우시안 분포 $\epsilon \sim \mathcal{N}(0, L^{-1})$</li>
    </ul>
  </li>
  <li>$p(y) = \mathcal{N}(y|A\mu + b, L^{-1}+A\Lambda^{-1}A^{T})$</li>
</ul>

<p>$p(y|x) = \mathcal{N}(y|Ax + b, L^{-1})$ 유도(공분산의 성질을 이용하여 y의 공분산을 계산)</p>

<p>\(\begin{align}
Cov(X+c)&amp;=Cov(X) (상수 벡터를 더해도 공분산은 변하지 않음) \\\\
Cov(X+Y)&amp;=Cov(X)+Cov(Y) (X와 Y가 독립일 경우) \\\\
Cov(AX)&amp;=A⋅Cov(X)⋅A^T \\\\
\end{align}\)
위 성질 사용
\(\begin{align}
Cov(y)&amp;=Cov(Ax+b+ϵ) \\\\
Cov(y)&amp;=Cov(Ax+ϵ)  (상수 b는 공분산에 영향을 주지 않음) \\\\
Cov(y)&amp;=Cov(Ax)+Cov(ϵ)  \\\\
Cov(y)&amp;=A⋅Cov(x)⋅A^T  +Cov(ϵ) \\\\
Cov(x)&amp;=Λ^{−1}(x의 공분산) \\\\
Cov(ϵ)&amp;=L^{-1} (노이즈의 공분산) \\\\
Cov(y)&amp;=AΛ^{−1}A^{T}+L^{−1} \\\\
\end{align}\)</p>

\[\begin{align}
p(y)=\mathcal{N}(y∣\underbrace{Aμ+b}_{평균}, \underbrace{L^{−1}+AΛ^{−1} A^{T}}_ {공분산})
\end{align}\]

<ul>
  <li>$p(y) \leftarrow q_\sigma(x_{t-1}|x_0)$</li>
  <li>$p(x) = \mathcal{N}(x|\mu, \Lambda^{-1})$</li>
  <li>$p(x) \leftarrow q_\sigma(x_t|x_0) = \mathcal{N}(\sqrt{\alpha_t}x_0, (1-\alpha_t)I)$</li>
  <li>$p(y|x) = \mathcal{N}(y|Ax+b,L^{-1})$</li>
  <li>$p(y|x) \leftarrow q_\sigma(x_{t-1}|x_t,x_0) = \mathcal{N} \left(\sqrt{a_{t-1}}x_0  + \sqrt{1-\alpha_{t-1}-\sigma^2_t} \cdot \frac{x_t - \sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}} , \sigma_t^2 I \right)$</li>
  <li>$q_\sigma(x_{t-1}|x_0) = \mathcal{N}(y|A\mu + b, L^{-1}+A\Lambda^{-1}A^T)$</li>
</ul>

\[\begin{align}
\mu &amp;= \sqrt{\alpha_t}x_0 \\\\
\Lambda^{-1} &amp;= (1-\alpha_t)I \\\\
A &amp;= \sqrt{1-\alpha_{t-1}-\sigma^2_t} \cdot \frac{1}{\sqrt{1-\alpha_t}} \\\\
b &amp;= \sqrt{\alpha_{t-1}}x_0 - \sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \frac{\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}} \\\\
L^{-1} &amp;= \sigma^2_tI
\end{align}\]

\[\begin{align}
A\mu+b &amp;= \sqrt{1-\alpha_{t-1}-\sigma^2_t} \cdot \frac{1}{\sqrt{1-\alpha_t}}\sqrt{\alpha_t}x_0 + \sqrt{\alpha_{t-1}}x_0 - \sqrt{1-alpha_{t-1}-\sigma_t^2} \cdot \frac{\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}} \\\
&amp;= \sqrt{\alpha_t}x_0 
\end{align}\]

\[\begin{align}
L^{-1} + A\Lambda^{-1}A^T &amp;= \sigma_t^2I + \sqrt{1-\alpha_{t-1}-\sigma^2_t} \cdot \frac{1}{\sqrt{1-\alpha_t}} (1-\alpha_t)I \left(\sqrt{1-\alpha_{t-1}-\sigma^2_t} \cdot \frac{1}{\sqrt{1-\alpha_t}} \right)^T \\\\
&amp;= \sigma_t^2I + (1-\alpha_{t-1}-\sigma_t^2)I \\\\
&amp;= (1-\alpha_{t-1})I
\end{align}\]

<ul>
  <li>최종식 유도</li>
</ul>

\[\begin{align}
q_\sigma(x_t\|x_0) &amp;= \mathcal{N}(\sqrt{\alpha_t}x0, (1-\alpha_t)I) \\\\
q_\sigma(x_{t-1}\|x_0) &amp;= \mathcal{N}(\sqrt{\alpha_{t-1}}x_0,(1-\alpha_{t-1})I) \\\\
&amp; 위에서 정의된 식 (DDPM) \\\\
q_\sigma(x_{t-1}\|x_t,x_0) &amp;= \mathcal{N}(x_{t-1}; \mu_q = A_tx_t+B_tx_0,\sigma^2_{*t}I) \;\; 로 정의 \\\\
p(y) &amp;= \mathcal{N}(y\|A\mu + b, L^{-1}+A\Lambda^{-1}A^{T}) \;\; 해당식을 활용하면 \\\\
\mu &amp;= \sqrt{\alpha_t}x_0 \\\\
\Lambda^{-1} &amp;= (1-\alpha_t)I \\\\
A &amp;= \sqrt{1-\alpha_{t-1}-\sigma^2_t} \cdot \frac{1}{\sqrt{1-\alpha_t}} \\\\
&amp;= A_t \\\\
b &amp;= \sqrt{\alpha_{t-1}}x_0 - \sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \frac{\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}} \\\\
b &amp;= B_tx_0 \\\\
L^{-1} &amp;= \sigma^2_tI \\\\
q_\sigma(x_{t-1}\|x_0) &amp;= \mathcal{N}(x_{t-1};\mu_q = \frac{\sqrt{1-\alpha_{t-1}-\sigma^2_t}}{\sqrt{1-\alpha_t}}\sqrt{\alpha_t}x_t + (\sqrt{\alpha_{t-1}} - \sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \frac{\sqrt{\alpha_t}}{\sqrt{1-\alpha_t}})x_0, \sigma_{*t}^2I) \\\\
&amp;= \mathcal{N}(x_{t-1};\mu_q = \sqrt{α_{t−1}}x_0 +  \sqrt{1−α_{t−1}−σ_t^2} \cdot \frac{x_t− \sqrt{α_t} x_0}{\sqrt{1−α_t}},σ_{*t}^2I) \;\; (7)
\end{align}\]

<ul>
  <li>Trained DDPM을 DDIM non-markovian에서 사용 가능 (retrain x )</li>
</ul>

<p>$q_\sigma(x_t|x_{t-1},x_0) \neq q_\sigma(x_t|x_{t-1})$</p>

<h4 id="sampling">Sampling</h4>

<ol>
  <li>
    <p>Goal: $x_t$에서 $x_{t-1}$을 만들고 싶다.</p>
  </li>
  <li>
    <p>Problem: 이상적인 방법(q)은 우리가 모르는 원본 이미지($x_0$)나 실제 노이즈(ε)를 필요로 해서 못 쓴다.</p>
  </li>
  <li>
    <p>Solution: q의 수식 형태를 본떠서, 실제 노이즈 ε를 신경망이 예측한 노이즈 $ε_θ$로 대체한 근사 모델 $p_θ$를 만든다.</p>
  </li>
  <li>
    <p>Sampling: $p_θ$라는 정규분포에서 샘플 $x_{t-1}$을 뽑기 위해 리파라미터라이제이션 트릭 (결과 = 평균 + 표준편차 × 랜덤값)을 사용한다.</p>
  </li>
</ol>

\[\begin{align}
q_\sigma(x_{t-1}\|x_0) &amp;= \mathcal{N}(\sqrt{α_{t−1}}x_0 +  \sqrt{1−α_{t−1}−σ_t^2} \cdot \frac{x_t− \sqrt{α_t} x_0}{\sqrt{1−α_t}},σ_{*t}^2I) \;\; (7) \\\\
q_\sigma(x_t\|x_0) &amp;= \mathcal{N}(\sqrt{\alpha_t}x_0, (1-\alpha_t)I) \\\\
x_t &amp;= \sqrt{\alpha_t}x_0 + \sqrt{1-\alpha_t}\epsilon \\\\
x_0 &amp;= \frac{x_t - \sqrt{1-\alpha_t}\epsilon}{\sqrt{\alpha_t}} \\\\
q_\sigma(x_{t-1}\|x_t,x_0) &amp;= \mathcal{N} \left(\sqrt{\alpha_{t-1}}\frac{x_t-\sqrt{1-\alpha_t}\epsilon}{\sqrt{\alpha_t}} + \sqrt{1-\alpha_{t-1}-\sigma^2_t}\epsilon,\sigma_t^2I \right) \\\\
p_\theta(x_{t-1}\|x_t) &amp;= \mathcal{N} \left(\sqrt{\alpha_{t-1}}\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta}{\sqrt{\alpha_t}} + \sqrt{1-\alpha_{t-1}-\sigma^2_t}\epsilon_\theta,\sigma_t^2I \right) \\\\
x_{t-1} &amp;= \sqrt{\alpha_{t-1}} \left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}} \right) + \sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_\theta^{(t)}(x_t)+\sigma_t\epsilon_t \;\; (Reparameterization - DDIM) \\\\
x_{t-1} &amp;= \frac{x_t}{\sqrt{\alpha_t}} - \frac{1-\alpha_t}{(\sqrt{1-\alpha_t})\sqrt{\alpha_t}}\epsilon_\theta^{(t)}(x_t) + \sigma_t\epsilon_t \;\; DDPM \; Sampling \; step \; (Comparison) \\\\
\end{align}\]

<ul>
  <li>$\sigma_t = \eta\sqrt{\frac{(1-\alpha_{t-1})}{(1-\alpha_t)}(1-\frac{\alpha_{t-1}}{\alpha_t})}$</li>
  <li>$\eta = 1 \rightarrow DDPM, \eta = 0 \rightarrow DDIM$</li>
</ul>

<h4 id="impact-of-variance-in-ddim">Impact of Variance in DDIM</h4>
<p>DDIM에서 정의하는 Variance (12) 
$\sigma_t^2 = \frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}$</p>

\[\begin{align}
&amp; DDIM x_{t-1} 일반화 \\\\
x_{t-1} &amp;= \sqrt{\alpha_{t-1}} \left( \underbrace{\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^t(x_t)}{\sqrt{\alpha_t}}}_{predicted \; x_0} \right) + \underbrace{\sqrt{1-\alpha_{t-1}-\sigma^2_t} \cdot \epsilon_\theta^{(t)}(x_t)}_{direction pointing \; to \; x_t} + \underbrace{\sigma_t\epsilon_t}_{random \; noise} \;\; (12) \\\\
&amp;DDPM 정의 \\\\
\tilde{\beta}_t &amp;= \frac{1-\alpha_{t-1}}{1-\alpha_t}\beta_t = \frac{1-\alpha_{t-1}}{1-\alpha_t} \left(1-\frac{\alpha_t}{\alpha_{t-1}} \right) \\\\
&amp; 분산(Variance) = \sigma_t^2 \\\\
&amp; DDPM = DDIM, \sigma_t^2 = \tilde{\beta_t} \\\\
\bar{\alpha_t} &amp;= \alpha_t \cdot \bar{\alpha_{t-1}} \\\\
\sigma_t^2 &amp;= \frac{1-\bar{\alpha_{t-1}}}{1-\bar{\alpha_t}} \left( 1- \frac{\bar{\alpha_t}}{\bar{\alpha_{t-1}}} \right) \\\\
&amp;=\frac{1-\bar{\alpha_{t-1}}}{1-\bar{\alpha_t}} \left( 1- \frac{\alpha_t \cdot \bar{\alpha_{t-1}}}{\bar{\alpha_{t-1}}} \right)\\\\
&amp;=\frac{1-\bar{\alpha_{t-1}}}{1-\bar{\alpha_t}} \left( 1- \alpha_t \right)\\\\
&amp;=\frac{(1-\alpha_t)(1-\bar{\alpha_{t-1}})}{1-\bar{\alpha_t}} \\\\
\end{align}\]

<ul>
  <li>(12)식 정리
\(\begin{align}
\sigma_t^2 &amp;= \frac{(1-\alpha_t)(1-\bar{\alpha_{t-1}})}{1-\bar{\alpha_t}} \\\\
\end{align}\)</li>
</ul>

\[\begin{align}
x_{t-1} &amp;= \sqrt{\alpha_{t-1}} \left( \frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^t(x_t)}{\sqrt{\alpha_t}} \right) + \sqrt{1-\alpha_{t-1}-\sigma^2_t} \cdot \epsilon_\theta^{(t)}(x_t) + \sigma_t\epsilon_t \;\; (12) \\\\
&amp;= \frac{x_t}{\sqrt{\alpha_t}}-\epsilon_\theta^{(t)}(x_t) \left( \sqrt{\frac{1-\bar{\alpha_t}}{\alpha_t}} - \sqrt{(1-\bar{\alpha}_{t-1}) - \frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}} \right) +\sigma_t\epsilon_t \\\\
&amp;= \frac{x_t}{\sqrt{\alpha_t}}-\epsilon_\theta^{(t)}(x_t) \left( \sqrt{\frac{1-\bar{\alpha_t}}{\alpha_t}} -\sqrt{\frac{(1-\bar{\alpha}_{t-1})(1-\bar{\alpha_t}) - (1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}} \right) + \sigma_t\epsilon_t \\\\
&amp;= \frac{x_t}{\sqrt{\alpha_t}}-\epsilon_\theta^{(t)}(x_t) \left( \sqrt{\frac{1-\bar{\alpha_t}}{\alpha_t}} -\sqrt{\frac{(1-\bar{\alpha}_{t-1})((1-\bar{\alpha}_t) - (1-\alpha_t))}{1-\bar{\alpha}_t}}\right) + \sigma_t\epsilon_t \\\\
&amp;= \frac{x_t}{\sqrt{\alpha_t}}-\epsilon_\theta^{(t)}(x_t) \left( \sqrt{\frac{1-\bar{\alpha_t}}{\alpha_t}} -\sqrt{\frac{(1-\bar{\alpha}_{t-1})(-\alpha_t\bar{\alpha}_{t-1} + \alpha_t)}{1-\bar{\alpha}_t}}\right) + \sigma_t\epsilon_t \\\\
\end{align}\]

\[\begin{align}
x_{t-1} &amp;= \frac{x_t}{\sqrt{\alpha_t}}-\epsilon_\theta^{(t)}(x_t) \left( \sqrt{\frac{1-\bar{\alpha_t}}{\alpha_t}} - \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{\sqrt{1-\bar{\alpha}_t}} \right) +\sigma_t\epsilon_t \\\\
&amp;= \frac{x_t}{\sqrt{\alpha_t}}-\epsilon_\theta^{(t)}(x_t) \left( \frac{1-\bar{\alpha}_t-\alpha_t+\bar{\alpha}_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}} \right) + \sigma_t\epsilon_t \\\\
&amp;= \frac{x_t}{\sqrt{\alpha_t}}-\epsilon_\theta^{(t)}(x_t) \left( \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}} \right) + \sigma_t\epsilon_t \\\\
\end{align}\]

\[\begin{align}

&amp; \text{DDPM  Sampling  Step} \\\\
x_{t-1} &amp;= \frac{x_t}{\sqrt{\alpha_t}} - \frac{(1-\alpha_t)}{(\sqrt{1-\bar{\alpha}_t})\sqrt{\alpha_t}}\epsilon_\theta^{(t)}(x_t)+\sigma_t\epsilon_t \\\\
\end{align}\]

<ul>
  <li>$\sigma_t^2$, DDPM = DDIM</li>
</ul>

\[\therefore q_\sigma(x_t\|x_{t-1},x_0) = q_\sigma(x_t\|x_{t-1})\]

<p>DDPM Posterior와 같은 분산으로 Non-Markovian process(DDIM)이 Markovian forward prcess(DDPM)로 일반화, DDPM과 같아진다</p>

<h4 id="accelerated-sampling-in-ddim">Accelerated Sampling in DDIM</h4>

\[\begin{align}
q_{\sigma}(x_{t-1}\|x_0) &amp;= \mathcal{N}(\sqrt{\alpha_{t-1}}x_0 + \sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \frac{x_t- \sqrt{\alpha_t} x_0}{\sqrt{1-\alpha_t}}, \sigma_{t}^2 I) \;\; \text{(7) DDIM} \\\\
q(x_{t-1}\|x_0) &amp;= \mathcal{N}(\sqrt{\alpha_{t-1}}x_0, (1-\alpha_{t-1})I) \;\; \text{DDPM} \\\\
q_{\sigma,\tau} (x_{1:T}\|x_0) &amp;= q_{\sigma,\tau}(x_{\tau_S}\|x_0) \prod^S_{i=1}q_{\sigma,\tau} (x_{\tau_{i-1}}\|x_{\tau_i}, x_0) \prod_{t\in\bar{\tau}}q_{\sigma,\tau} (x_t\|x_0) \;\; \text{(52)} \\\\
&amp; \tau \text{ is a sub-sequence of } [1, \dots, T] \text{ of length S with } \tau_S = T, \{x_{\tau_1}, \dots, x_{\tau_s}\} \\\\
&amp; \bar{\tau} := \{1, \dots, T\} \setminus \tau \\\\
q_{\sigma,\tau}(x_t\|x_0) &amp;= \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I) \quad \forall t \in \bar{\tau} \cup \{T\} \\\\
q_{\sigma,\tau}(x_{\tau_i}\|x_0) &amp;= \mathcal{N}(\sqrt{\alpha_{\tau_i}}x_0, (1-\alpha_{\tau_i})I) \quad \forall i \in [S] \;\;(54) \\\\
q_{\sigma,\tau}(x_{\tau_{i-1}}\|x_{\tau_i}, x_0) &amp;= \mathcal{N}\left(\sqrt{\alpha_{\tau_{i-1}}}x_0 + \sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_i}^2} \cdot \frac{x_{\tau_i} - \sqrt{\alpha_{\tau_i}} x_0}{\sqrt{1-\alpha_{\tau_i}}}, \sigma_{\tau_i}^2 I\right) \quad \forall i \in [S] \;\; \text{(7) DDIM} \\\\
\end{align}\]

\[\begin{align}
&amp; p_{\theta}(x_{0:T}) \text{"는 가속 샘플링을 위한 생성과정(Generative Process)를 수학적으로 정의"} \\\\
p_{\theta}(x_{0:T}) &amp;:= \underbrace{p_{\theta}(x_T)\prod^S_{i=1}p^{(\tau_i)}_{\theta}(x_{\tau_{i-1}}\|x_{\tau_i})}_{\text{"use to produce samples"}} \times \underbrace{\prod_{t \in \bar{\tau}}p^{(t)}_{\theta}(x_0\|x_t)}_{\text{"in variational objective"}} \;\; (55) \\\\
&amp;= p_\theta(x_T) \text{"사전 분포 (Prior Distribution)"} x \text{"Sampling Path"} x \text{"변분 목적 함수 (Variational Objective Term)"} \\\\
\end{align}\]

<p>We consider two types of selection procedure for τ given the desired dim($τ$) &lt; T:</p>

<ul>
  <li>Linear: we select the timesteps such that $τ_i$ = $[ci]$ for some c;</li>
  <li>Quadratic: we select the timesteps such that $τ_i$ = $[ci^2]$ for some c.</li>
</ul>

<h3 id="result">Result</h3>
<ul>
  <li>Lower is Better
<img width="1103" height="387" alt="image" src="https://github.com/user-attachments/assets/d26245a4-fc48-4ea5-8407-b63c3bf30d5e" /></li>
</ul>

<h2 id="diffusion-models-as-score-based-models-vs-stochastical-differential-equationsde">Diffusion Models as Score Based Models vs Stochastical Differential Equation(SDE)</h2>

<h3 id="score-based-model">Score Based Model</h3>
<ul>
  <li>
    <p>Given Probability p(x)가 주어졌을때, Score는 $\nabla_xlog(p(x))$ (Gradient and Log density function)</p>
  </li>
  <li>
    <p>이 Gradient Log Density function은 Direction을 나타낸다 (Noise $\rightarrow$ Image)</p>
  </li>
</ul>

<p><img width="500" height="465" alt="image" src="https://github.com/user-attachments/assets/d3c5cece-1530-4868-ba96-5db420d7edae" /></p>

<p><img width="934" height="545" alt="image" src="https://github.com/user-attachments/assets/6514dcfb-364a-42a2-a5a1-ae3bcd3f9deb" /></p>

<ul>
  <li>Gradient 방향으로 움직여주면, High Likelyhood로 모인다.</li>
</ul>

<p><img width="907" height="543" alt="image" src="https://github.com/user-attachments/assets/9e313d2b-b9b8-4812-a1c9-3c4cdc976541" /></p>

<ul>
  <li>Local optimal 에 모일수도 있다.</li>
</ul>

\[\begin{align}
s_\theta(x) \approx \nabla_xlog(p(x)) \\\\
\frac{1}{2}E_{x\sim p_data} \parallel \nabla_x log(p_{data}(x)) - s_\theta(x) \parallel^2_2 \\\\
\text{Minimizing Euclidean Distance between Data Score x and Estimated Score x} \\\\
E_{x\sim p_data} \left[ \frac{1}{2} \parallel s_\theta(x) \parallel^2_2 + tr(\nabla_xs_\theta(x)) \right] \\\\
tr = Trace \text{대각합} 
\end{align}\]

<ul>
  <li>
    <p>대각합으로 변환한 이유 ($∇ₓlog(p_{data}(x))$ 계산할 방법이 없어)</p>

    <ul>
      <li>문제의 근원: $p_{data}(x)$의 정체를 모른다는 것</li>
      <li>우리가 학습하려는 데이터의 실제 확률 분포 $p_{data}(x)$ 의 정확한 함수식을 모른다는 점</li>
      <li>우리가 가진 것은 $p_{data}(x)$ 라는 함수 자체가 아니라, 그 분포에서 추출된 샘플(sample)들의 집합 뿐</li>
      <li>함수식을 모르기 때문에, 당연히 $log(p_{data}(x))$를 계산할 수 없고, 그것을 미분한 $∇ₓlog(p_{data}(x))$ (스코어) 역시 절대 직접 계산할 수 없습니다.</li>
      <li>따라서, 원래의 목적 함수 $\parallel ∇ₓlog(p_{data}(x)) - s_θ(x)\parallel_2^2$는 이론적으로는 완벽하지만 실제로는 <strong>계산이 불가능한 ‘그림의 떡’</strong>인 셈입니다.</li>
    </ul>
  </li>
  <li>
    <p>수학적 돌파구: ‘부분적분’을 이용한 미분 옮기기</p>
    <ul>
      <li>부분적분 공식은 다음과 같습니다.</li>
      <li>$\int u(x)v′(x)dx=u(x)v(x)−\int u′(x)v(x)dx$</li>
      <li>$E_{x∼p_{data}}[복잡한 항]≈ \int p_{data}(x)⋅(복잡한 항)dx$</li>
      <li>$p_{data}(x)$를 u역할로 봅니다. 모델과 관련된 나머지 부분을 v’ 역할로 봅니다.</li>
    </ul>
  </li>
</ul>

\[\begin{align}
L(\theta) &amp;= \frac{1}{2}E_{x\sim p_data} \parallel \nabla_x log(p_{data}(x)) - s_\theta(x) \parallel^2_2 \\\\
&amp; \text{적분형태} \\\\
&amp;= \frac{1}{2} \int p_{data}(x) \parallel \nabla_x log(p_{data}(x)) - s_\theta(x) \parallel^2_2dx \\\\
&amp; \text{제곱 항 전개} \\\\
&amp;= \frac{1}{2} \int p_{data}(x) \text{[} \parallel \nabla_x log(p_{data}(x)) \parallel^2 -2(\nabla_xlogp_{data})^Ts_\theta + \parallel s_\theta(x) \parallel^2 \text{]}dx \\\\
&amp;= \underbrace{\frac{1}{2} \int p_{data}(x) \parallel \nabla_x log(p_{data}(x)) \parallel^2 dx}_{\text{1 항}} - \underbrace{\int p_{data}(\nabla_xlogp_{data})^Ts_\theta dx}_{\text{2 항}} + \underbrace{\frac{1}{2}\int p_{data} \parallel s_\theta(x) \parallel^2dx}_{\text{3 항}} \\\\
&amp; \text{1 항} \theta \text{ 와 무관하므로 상수} \\\\
&amp; \text{2 항은 계산 불가의 } \nabla_x \log p_{data} \text{ 를 포함, 우리가 부분적분할 대상} \\\\
&amp; \text{3 항 } s_\theta \text{ 에 대한 항, 계산 가능} \\\\
&amp; \text{2 항} \\\\
&amp; - \int p_{data}(x)(\nabla_x log p_{data}(x))^T s_\theta(x)dx \\\\
\nabla_x log(f(x)) &amp;= (\nabla_x f(x)) / f(x) \text{미분 트릭 적용} \\\\
&amp; - \int p_{data}(x) \left(\frac{\nabla_x log p_{data}(x)}{p_{data}(x)} \right)^T s_\theta(x)dx \\\\
&amp; - \int (\nabla_x p_{data}(x))^Ts_\theta(x)dx \\\\
&amp; \text{다차원 부분적분} \int (\nabla f)^T g dx = - \int f (\nabla \cdot g) dx \\\\
&amp; + \int p_{data}(x)tr(\nabla_x s_\theta (x))dx \\\\
L(\theta) &amp;= (상수) + \int  p_{data}(x)tr(\nabla_x s_\theta (x))dx  + \frac{1}{2}\int p_{data} \parallel s_\theta(x) \parallel^2dx \\\\
&amp;= E_{x\sim p_{data}} \left[tr(\nabla_x s_\theta(x)) + \frac{1}{2}\parallel s_\theta (x) \parallel^2 \right] + 상수 \\\\
\end{align}\]

<h3 id="trnabla_x-s_thetax-계산이-computatively-expensive">$tr(\nabla_x s_\theta(x))$ 계산이 computatively expensive</h3>

<ul>
  <li>
    <p>“모델이 깨끗한 데이터의 스코어를 배우게 하는 대신, 약간의 노이즈를 섞은 데이터의 스코어를 배우게 하면 더 안정적이고 효과적이지 않을까?”</p>
  </li>
  <li>
    <p>Original Score Matching $\rightarrow$ Denoising Score Matching</p>
  </li>
  <li>
    <p>이제 모델 $s_θ$는 깨끗한 데이터 x가 아닌, 노이즈 낀 데이터 $x̃$ 를 입력받습니다.</p>
  </li>
  <li>
    <p>목표 스코어도 $∇ₓlog(p_{data}(x))$ 가 아닌, 노이즈 낀 데이터의 분포 $q_σ(x̃)$ 의 스코어인 $∇_{x̃}log(q_σ(x̃))$ 로 바뀌었습니다.</p>
  </li>
</ul>

\[\begin{align}
L(\theta) &amp;= E_{x\sim p_{data}} \left[\frac{1}{2}\parallel s_\theta (x) \parallel_2^2  +  \text{tr}(\nabla_x s_\theta(x)) \right]\\\\
&amp; q_\sigma(\tilde{x}) \rightarrow \text{Noise가 추가된 x의 q 확률 밀도함수} \\\\
&amp;q_\sigma(\tilde{x}) = \int q_\sigma(\tilde{x}\|x) p_{data}(x) dx \\\\
&amp;= \frac{1}{2}E_{\tilde{x} \sim q_{\sigma}} \left[ \parallel \nabla_{\tilde{x}}\log q_\sigma(\tilde{x}) - s_\theta(\tilde{x}) \parallel_2^2 \right] \\\\
\end{align}\]

<h3 id="x̃logq_σx̃를-계산하려면-여전히-pdata를-알아야-함-즉-또다시-계산-불가능한-문제">$∇<em>{x̃}log(q_σ(x̃))$를 계산하려면 여전히 $p</em>{data}$를 알아야 함. 즉, 또다시 계산 불가능한 문제</h3>
<ul>
  <li>
    <p>핵심은 목표 스코어를 $∇<em>{\tilde{x}}log(q_σ(x̃))$에서 $∇</em>{\tilde{x}}log(q_σ(\tilde{x}|x))$로 바꾼 것입니다.</p>
  </li>
  <li>
    <p>계산 가능한 목표 (✅): $q_σ(\tilde{x}|x)$는 “깨끗한 데이터 x가 주어졌을 때, 노이즈 낀 데이터 $\tilde{x}$가 나올 확률”입니다. 이것은 우리가 직접 정의하는 간단한 가우시안 분포 $N(\tilde{x}|x,σ²I)$입니다.</p>
  </li>
  <li>
    <p>우리는 이 함수의 정확한 식을 알고 있기 때문에, 스코어 $∇_{\tilde{x}}log(q_σ(\tilde{x}|x))$를 쉽게 계산할 수 있습니다. 그 결과가 바로 $-(\tilde{x}-x)/σ²$ 입니다.</p>
  </li>
</ul>

<h3 id="x̃logq_σx̃-rightarrow-x̃logq_σx̃x">$∇<em>{x̃}log(q_σ(x̃)) \rightarrow ∇</em>{x̃}log(q_σ(x̃|x))$</h3>

\[∇_\tilde{x} log q_σ(\tilde{x})=E_{x∼q(x∣\tilde{x})}[∇_\tilde{x} log q_σ(\tilde{x}∣x)]\]

<ul>
  <li>손실 함수 L(θ)를 파라미터 θ로 미분한 값, 즉 **기울기 $∇<em>θ L(θ)$**를 사용합니다. 만약 두 손실 함수의 기울기가 같다면 $(∇_θ L</em>{hard} = ∇<em>θ L</em>{easy})$, 두 함수를 최적화하는 것은 완벽하게 동일한 과정이 됩니다.
\(\begin{align}
∇_θL_{hard} &amp;= E_\tilde{x}[−2(∇_\tilde{x}log q_σ(\tilde{x}) − s_θ(\tilde{x}))∇_θs_θ(\tilde{x})] \\\\
&amp;=E_\tilde{x}[−2(E_{x\sim q(x\|\tilde{x})} [∇_\tilde{x}log q_σ(\tilde{x}\|x)] − s_θ(\tilde{x}))∇_θs_θ(\tilde{x})] \\\\
&amp;=E_\tilde{x}[−2(∇_\tilde{x}log q_σ(\tilde{x}\|x) − s_θ(\tilde{x}))∇_θs_θ(\tilde{x})] = \nabla_\theta L_{easy} \\\\ 
\end{align}\)</li>
</ul>

<h3 id="tildex--xepsilon-noise">$\tilde{x} = x+\epsilon (noise)$</h3>

\[\begin{align}
L_\theta &amp;= \frac{1}{2}E_{\tilde{x} \sim q_{\sigma}} \left[ \parallel \nabla_{\tilde{x}}\log q_\sigma(\tilde{x}) - s_\theta(\tilde{x}) \parallel_2^2 \right] \\\\
&amp;= \frac{1}{2}E_{x\sim P_{data}, \\ \tilde{x} \sim q_{\sigma}(\tilde{x}\|x)} \left[ \parallel \nabla_{\tilde{x}}\log q_\sigma(\tilde{x}\|x) - s_\theta(\tilde{x}) \parallel_2^2 \right] \\\\
&amp;= \frac{1}{2}E_{x\sim P_{data}, \\ \tilde{x} \sim q_{\sigma}(\tilde{x}\|x)} \left[ \parallel -\frac{\epsilon}{\sigma^2} - s_\theta(\tilde{x}) \parallel_2^2 \right] \\\\
\end{align}\]

<h3 id="score-based-model-1">Score Based Model</h3>

<p><img width="1105" height="660" alt="image" src="https://github.com/user-attachments/assets/5af61fda-8ba3-4c4b-8789-f8657c7a6026" /></p>

<ul>
  <li>Noise conditional score Sampling is very similar to Diffusion reverse process</li>
</ul>

<p><img width="1119" height="447" alt="image" src="https://github.com/user-attachments/assets/b98051d7-1005-4a6d-bb97-11284db77dbc" /></p>

<hr />

<h3 id="stochastic-differential-equation">Stochastic Differential Equation</h3>
<p>\(dx = f(x, t)dt + g(t)dw\)</p>

<p>시간이 아주 미세하게 ($dt$) 변할때, 데이터 x가 얼마나 미세하게 ($dx$) 변하는지 설명</p>

<ul>
  <li>$f(x,t)dt$ : Drift 항
    <ul>
      <li>데이터 x와 시간 t에 따라 데이터가 어떤 정해진 방향으로 미세하기 움직이도록</li>
    </ul>
  </li>
  <li>$g(t)dw$ : Diffusion 항
    <ul>
      <li>무작위적인 부분 담당</li>
    </ul>
  </li>
</ul>

\[\begin{align}
q(x_t\|x_{t−1}) &amp;:= N (x_t;\sqrt{1 − β_t}x_{t−1}, β_tI \;\; \text{(DDPM Transition, Forward)} \\\\
&amp; x_t\text{정의} \\\\
x_{t} &amp;= \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}z_t \\\\
&amp; \sqrt{1-\beta_t} \approx 1- \frac{\beta_t}{2}  \;\; \text{(approximate 사용)} \\\\
dx &amp;= -\frac{1}{2}\beta(t)xdt + \sqrt{\beta(t)}dw \;\; \text{(DDPM SDE)} \\\\
dx &amp;= [f(x,t) - g(t)^2\nabla_xlogp_t(x)]dt + g(t)d\bar{w} \;\; \text{(General Form of Reverse SDE)}  \\\\
dx &amp;= [-\frac{1}{2} \beta(t)x - \beta(t) \nabla_xlogp_t(x)]dt + \sqrt{\beta(t)}dw  \text{(General to DDPM SDE)} \\\\
&amp; \frac{1}{\sqrt{1-\beta_t}} \approx 1+\frac{\beta_t}{2} \;\; \text{(approximate 사용하여 } x_{t-1} \text{ 표현)} \\\\
x_{t-1} &amp;= \frac{1}{\sqrt{\alpha_t}}\left(x_{t-1} -\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t)\right) + \sigma_tz \;\; \text{(DDPM Sampling algorithm 2)}\\\\
&amp;= \frac{1}{\sqrt{1-\beta_t}}(x_t+\beta_t s_\theta(x_t)) + \sqrt{\beta_t}z_t \\\\
&amp; x_{t-1} 유도 \\\\

% Definitions
\alpha_t &amp;= 1 - \beta_t \\\\
s_{\theta}(x_t) &amp;= -\frac{1}{\sqrt{1-\bar{\alpha}_t}} \epsilon_{\theta}(x_t, t) \\\\
\epsilon_{\theta}(x_t, t) &amp;= -\sqrt{1-\bar{\alpha}_t} \cdot s_{\theta}(x_t) \\\\
% Derivation
x_{t-1} &amp;= \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_{\theta}(x_t, t)\right) + \sigma_t z \\\\
&amp;= \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \left(-\sqrt{1-\bar{\alpha}_t} \cdot s_{\theta}(x_t)\right)\right) + \sigma_t z \\\\
&amp;= \frac{1}{\sqrt{\alpha_t}}\left(x_t + (1-\alpha_t)s_{\theta}(x_t)\right) + \sigma_t z \\\\
&amp;= \frac{1}{\sqrt{1-\beta_t}}\left(x_t + \beta_t s_{\theta}(x_t)\right) + \sigma_t z \\\\
&amp;= \frac{1}{\sqrt{1-\beta_t}}\left(x_t + \beta_t s_{\theta}(x_t)\right) + \sqrt{\beta_t} z_t
\end{align}\]

<h4 id="x_t-1---x_t-수학적-근사-rightarrow-reverse-sde-근사">$x_{t-1} - x_t$ 수학적 근사 $\rightarrow$ Reverse SDE 근사</h4>

\[\begin{align*}
d\mathbf{x} &amp;= \mathbf{f}(\mathbf{x}, t)dt + g(t)d\mathbf{w} \text{  (Forward SDE)}\\\\
d\mathbf{x} &amp;= \left[\mathbf{f}(\mathbf{x}, t) - \frac{1}{2}g(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right] dt \text{  (Reverse SDE)} \\\\
dx &amp;= -\frac{1}{2}\beta(t)x \, dt + \sqrt{\beta(t)} \, dw \quad \text{  (DDPM Forward SDE)} \\\\
&amp; \frac{1}{\sqrt{1-\beta_t}} \approx 1 + \frac{\beta_t}{2} \quad \text{as } \beta_t \to 0  \\\\
x_{t-1} &amp;= \sqrt{\bar{\alpha}_{t-1}}\left(\frac{x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\bar{\alpha}_t}}\right) + \sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \cdot \epsilon_\theta^{(t)}(x_t) + \sigma_t \epsilon_t \text{  (DDIM 의 일반적 Sampling 알고리즘)} \\\\
x_{t-1} &amp;= \frac{x_t}{\sqrt{1-\beta_t}} - \left(\sqrt{\frac{1-\bar{\alpha}_t}{1-\beta_t}} - \sqrt{1-\bar{\alpha}_{t-1}}\right)\epsilon_\theta(x_t)  \;\; (\alpha_t = 1-\beta_t \text{  를 활용하여 정리)} \\\\
x_{t-1} &amp;= \frac{x_t}{\sqrt{1-\beta_t}} - \left(\sqrt{\frac{1-\bar{\alpha}_t}{1-\beta_t}} - \sqrt{\frac{1-\bar{\alpha}_t}{1-\beta_t}}\right)\epsilon_\theta(x_t) \\\\
x_{t-1} &amp;= \left(1+\frac{\beta_t}{2}\right)x_t - \frac{\beta_t}{2\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t) \\\\
x_{t-1}-x_t &amp;= \left(\frac{\beta_t}{2}\right)x_t - \frac{\beta_t}{2\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t)
\end{align*}\]

<ul>
  <li>
    <p>$x_{t-1} - x_t  ↔  dx$ (미세한 변화량)</p>
  </li>
  <li>
    <p>알고리즘의 $(β_t/2)x_t$ 항  ↔  역방향 SDE의 드리프트 항</p>
  </li>
  <li>
    <p>알고리즘의 $ε_θ$ 항  ↔  역방향 SDE의 스코어 $(∇ₓlog p_t(x))$ 항</p>
  </li>
  <li>
    <p>SDE를 푸는 더 발전된 수치해석 기법을 도입하여 샘플링 속도나 품질을 개선하는 연구가 가능</p>
  </li>
</ul>

<p>\(\begin{align}
dx &amp;= [f(x,t) - g(t)^2\nabla_xlogp_t(x)]dt + g(t)d\bar{w} \;\; \text{(General Form of Reverse SDE)}  \\\\
d\mathbf{x} &amp;= \left[\mathbf{f}(\mathbf{x}, t) - \frac{1}{2}g(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right] dt \text{  (Reverse SDE)} \\\\
\end{align}\)</p>
<ul>
  <li>1/2이 있는 식은 확률 흐름 ODE(Probability Flow ODE)이고, 1/2이 없는 식은 일반적인 역방향 SDE(Reverse SDE)</li>
</ul>

<p>순방향 SDE dx = f dt + g dw를 거꾸로 되돌리는 방법에는 크게 두 가지가 있습니다. (noise 유무)</p>

<ol>
  <li>역방향 SDE (Reverse SDE)</li>
</ol>

<p>수식: $dx = [f(x,t) − g(t)²∇ₓlog p_t(x)]dt + g(t)dw̄$</p>

<ol>
  <li>확률 흐름 ODE (Probability Flow ODE)</li>
</ol>

<p>수식: $dx = [f(x, t) - (1/2)g(t)²∇ₓlog p_t(x)]dt$</p>

<hr />

<h3 id="appendix">Appendix</h3>
<h4 id="l2-norm">L2 Norm</h4>
<p>$\parallel ⋅ \parallel_2$ : L2 노름 (Euclidean Norm)
이 기호는 <strong>L2 노름(norm)</strong>을 나타내며, 벡터의 ‘크기’ 또는 ‘길이’를 측정하는 가장 일반적인 방법입니다. 우리가 보통 생각하는 두 점 사이의 직선 거리를 계산하는 것과 같습니다.</p>

<p>예를 들어, 2차원 벡터 v = (x, y)가 있다면, L2 노름은 다음과 같이 계산됩니다.</p>

\[\parallel v \parallel_2 = \sqrt{x^2+y^2}\]

\[\parallel v \parallel_2^2 = x^2 + y^2\]

<p>​</p>
<h4 id="_tildexlogq_σtildexx-rightarrow--tildex-xσ">$∇_{\tilde{x}}log(q_σ(\tilde{x}|x)) \rightarrow -(\tilde{x}-x)/σ²$</h4>

<ul>
  <li>확률밀도 함수
\(\begin{align}
q_{\sigma}(\tilde{x}\|x) &amp;= \frac{1}{\sqrt{(2\pi)^D \|\sigma^2 I\|}} \exp\left(-\frac{1}{2}(\tilde{x}-x)^T(\sigma^2 I)^{-1}(\tilde{x}-x)\right) \\\\
\end{align}\)</li>
  <li>Log
    <ul>
      <li>$(σ²I)⁻¹$는 역행렬이므로 $(1/σ²)I$가 됩니다.</li>
      <li>$(x̃-x)ᵀ I (x̃-x)$는 벡터 $(x̃-x)$의 내적(dot product)이므로, 제곱 L2 노름 $\parallel \bar{x}-x\parallel^2$와 같습니다.
\(\begin{align}
\log q_{\sigma}(\tilde{x}\|x) &amp;= \text{상수} - \frac{1}{2}(\tilde{x}-x)^T(\sigma^2 I)^{-1}(\tilde{x}-x) \\\\
&amp;= \text{상수} - \frac{1}{2\sigma^2} \parallel \tilde{x} - x \parallel_2^2 \\\\
\end{align}\)</li>
    </ul>
  </li>
  <li>$∇_{x̃}$ 미분 계산
    <ul>
      <li>∑ 안쪽의 $(x̃ᵢ - xᵢ)²$ 항을 $x̃ᵢ$에 대해 편미분하면, 체인룰(chain rule)에 의해 $2(x̃ᵢ - xᵢ)$가 됩니다.</li>
      <li>x̃와 x는 단순히 하나의 숫자가 아니라, 여러 개의 숫자로 이루어진 벡터 $\rightarrow \ \sum$
\(\begin{align}
\nabla_{\tilde{x}} \log q_{\sigma}(\tilde{x}\|x) &amp;= \nabla_{\tilde{x}} \left[ \text{상수} - \frac{1}{2\sigma^2} \sum_i (\tilde{x}_i - x_i)^2 \right] \\\\
&amp;= - \frac{1}{2\sigma^2} \cdot \left[ 2(\tilde{x}_1 - x_1), 2(\tilde{x}_2 - x_2), \dots \right] \\\\
&amp;= - \frac{1}{\sigma^2} \left[ \tilde{x}_1 - x_1, \tilde{x}_2 - x_2, \dots \right] \\\\
&amp;= - \frac{\tilde{x} - x}{\sigma^2}
\end{align}\)</li>
    </ul>
  </li>
</ul>

      </div>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#test">test</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  

  

  
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=%2F2025-08-21-%25EB%2585%25BC%25EB%25AC%25B8%25EB%25A6%25AC%25EB%25B7%25B0-Denoising-Diffusion-Implicit-Models%28DDIM%29%2F"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fab fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

  

  

</section>



      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Deep-Unsupervised-Learning-using-Nonequilibrium-Thermodynamics-(2015-ICML)/" data-toggle="tooltip" data-placement="top" title="[논문리뷰]Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015 ICML)">
            <i class="fas fa-arrow-left" alt="Previous Post"></i>
            <span class="d-none d-sm-inline-block">Previous Post</span>
          </a>
        </li>
        
        
      </ul>
      
  
  
  

  


  

  



    </div>
  </div>
</main>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      
<ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:younghyeok25@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/https://github.com/james-hyeok-kim/james-hyeok-kim.github.io.git" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/https://www.linkedin.com/in/jameskim525" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li></ul>


      
      <p class="copyright text-muted">
      
        James kim
        &nbsp;&bull;&nbsp;
      
      2025

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="/">MyWebsite.com</a>
        </span>
      

      

      

      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>

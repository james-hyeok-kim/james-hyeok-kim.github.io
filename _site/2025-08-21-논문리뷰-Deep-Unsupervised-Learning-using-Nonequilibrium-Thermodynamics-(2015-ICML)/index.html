<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 6.0.1 | Copyright Dean Attali 2023 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  

  <title>[논문리뷰]Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015 ICML) | AI 리부팅 (AI Rebooting)</title>

  
  
  <meta name="author" content="James Kim">
  

  <meta name="description" content="Diffusion">

  

  

  
  <link rel="alternate" type="application/rss+xml" title="AI 리부팅 (AI Rebooting)" href="/feed.xml">
  

  

  

  

  

  
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script"></script>



  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="AI 리부팅 (AI Rebooting)">
  <meta property="og:title" content="[논문리뷰]Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015 ICML) | AI 리부팅 (AI Rebooting)">
  <meta property="og:description" content="Diffusion">

  
  <meta property="og:image" content="/assets/img/AI.jpg">
  

  
  <meta property="og:type" content="article">
  
  <meta property="og:article:author" content="James Kim">
  
  <meta property="og:article:published_time" content="2025-08-21T00:00:00-04:00">
  <meta property="og:url" content="/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Deep-Unsupervised-Learning-using-Nonequilibrium-Thermodynamics-(2015-ICML)/">
  <link rel="canonical" href="/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Deep-Unsupervised-Learning-using-Nonequilibrium-Thermodynamics-(2015-ICML)/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="[논문리뷰]Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015 ICML) | AI 리부팅 (AI Rebooting)">
  <meta property="twitter:description" content="Diffusion">

  
  <meta name="twitter:image" content="/assets/img/AI.jpg">
  

  


  

  
  

  

</head>


<body>
  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="/">AI 리부팅 (AI Rebooting)</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/about%20me">About Me</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://www.linkedin.com/in/jameskim525">Author's home</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="/">
          <img alt="Navigation bar avatar" class="avatar-img" src="/assets/img/AI.jpg" />
        </a>
      </div>
    </div>
  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: '/assets/data/searchcorpus.json' 
    });
  </script>
</div>





  



<header class="header-section ">
<div class="intro-header ">
  
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>[논문리뷰]Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015 ICML)</h1>
          
            
              <h2 class="post-subheading">Diffusion</h2>
            
          
          
           
            
              By <strong>James Kim</strong><br>
            
            <span class="post-meta">Posted on August 21, 2025</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
  
  
</div>



</header>


<main class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      
        
        
        

        <div id="header-gh-btns">
          
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=james-hyeok-kim&repo=james-hyeok-kim.github.io.git&type=star&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=james-hyeok-kim&repo=james-hyeok-kim.github.io.git&type=fork&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=james-hyeok-kim&type=follow&count=true" frameborder="0" scrolling="0" width="220px" height="20px"></iframe>
              
            
          
        </div>
      

      

      <div class="blog-post">
        <h2 id="deep-unsupervised-learning-using-nonequilibrium-thermodynamics">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</h2>
<hr />

<ul>
  <li>저자(소속) :
    <ul>
      <li>Jascha Sohl-Dickstein (Stanford University)</li>
      <li>Eric A. Weiss(University of California, Berkeley)</li>
      <li>Niru Maheswaranathan (Stanford University)</li>
      <li>Surya Ganguli(Stanford University)</li>
    </ul>
  </li>
  <li>
    <p>논문 : <a href="https://arxiv.org/pdf/1503.03585">PDF</a></p>
  </li>
  <li>일자 : 18 Nov 2015, ICML(International conference on machine learning)</li>
</ul>

<hr />

<h3 id="1-introduction">1. Introduction</h3>

<h4 id="11-문제-설정-tractability-vs-flexibility">1.1 문제 설정: Tractability vs Flexibility</h4>

<ul>
  <li>배경 문제: 머신러닝에서는 복잡한 데이터 분포를 잘 모델링하는 것이 목표입니다.
    <ul>
      <li>그러나 대부분의 확률 모델은 다음 두 가지 중 하나를 만족하기는 쉽지만, 동시에 만족하기는 어렵습니다:</li>
    </ul>
  </li>
  <li>
    <p>Tractable: 수학적으로 계산이 가능해야 함 (예: 정규분포처럼 확률 계산이 쉬운 모델)</p>
  </li>
  <li>Flexible: 복잡하고 다양한 분포를 표현할 수 있어야 함
(예: 어떤 함수 $φ(x)$든지 가능한 분포 $𝑝(𝑥) = \frac{𝜙(𝑥)}{𝑍}$)
    <ul>
      <li>정규화된 확률분포의 일반적인 형태</li>
      <li>$𝜙(𝑥)$: 어떤 양의 함수 (unnormalized probability)</li>
      <li>$𝑍=∫𝜙(𝑥)𝑑𝑥$: 정규화 상수 (partition function)</li>
    </ul>
  </li>
  <li>
    <p>문제점: 유연한 모델은 정규화 상수 
$Z=∫ϕ(x)dx$를 계산하기가 거의 불가능합니다.</p>
  </li>
  <li>샘플링이나 학습도 Monte Carlo 방식으로 매우 비쌈.</li>
</ul>

<h5 id="111-기존-해결-시도들--이런-방법들은-트레이드오프를-완전히-해결하지는-못합니다">1.1.1 기존 해결 시도들 → 이런 방법들은 트레이드오프를 완전히 해결하지는 못합니다.</h5>
<ul>
  <li>Variational Bayes</li>
  <li>Contrastive Divergence</li>
  <li>Score Matching</li>
  <li>Pseudolikelihood</li>
  <li>Loopy Belief Propagation</li>
  <li>Mean Field Theory</li>
  <li>Minimum Probability Flow</li>
</ul>

<h4 id="12-논문의-핵심-아이디어">1.2 논문의 핵심 아이디어</h4>
<p>이 논문에서는 물리학(특히 비평형 통계물리학 nonequilibrium thermodynamics)에서 아이디어를 차용하여 다음과 같은 새로운 방법을 제안합니다:</p>

<h5 id="121-핵심-개념-확산-프로세스-기반-생성-모델">1.2.1 핵심 개념: 확산 프로세스 기반 생성 모델</h5>
<ul>
  <li>데이터를 점점 노이즈화하는 Forward Diffusion Process를 정의하고,</li>
  <li>그것을 역으로 되돌리는 Reverse Diffusion Process를 학습하여,</li>
  <li>데이터 분포를 생성할 수 있는 모델로 삼습니다.</li>
</ul>

<h5 id="122-모델의-장점">1.2.2 모델의 장점:</h5>
<ul>
  <li>유연한 구조: 수천 개의 레이어(타임스텝)도 사용 가능.</li>
  <li>샘플링이 정확: 각 단계가 tractable한 확률분포라 전체 샘플링도 tractable.</li>
  <li>확률 계산이 쉬움: Likelihood와 Posterior 계산이 효율적.</li>
  <li>다른 분포와의 곱셈이 쉬움: 예를 들어 Posterior 계산시 조건부 분포와 곱하기가 가능.</li>
</ul>

<hr />

<h3 id="2-algorithm">2. Algorithm</h3>

<h4 id="목표">목표</h4>
<ul>
  <li>
    <p>Forward diffusion process를 통해 복잡한 데이터 분포를 <strong>단순한 분포(예: 정규분포)</strong>로 변환</p>
  </li>
  <li>
    <p>Reverse diffusion process를 학습하여, 단순한 분포로부터 데이터를 복원하는 생성 모델을 정의</p>
  </li>
  <li>
    <p>이 과정은 물리학에서 말하는 <strong>비평형 확산(nonequilibrium diffusion)</strong>에 기반하며, 확률모델 자체를 Markov chain의 종단 상태로 정의합니다.</p>
  </li>
</ul>

<hr />

<h4 id="21-forward-trajectory-inference-process">2.1 Forward Trajectory (Inference Process)</h4>

<h5 id="211-핵심-개념">2.1.1 핵심 개념:</h5>
<p>데이터 분포 $q(x^(0))$로부터 시작해서, 확산 커널을 반복 적용하여 점점 구조를 무너뜨려 단순한 분포 $π(x^{T})$로 만듭니다.</p>

\[q(x^{(t)}∣x^{(t−1)})=T_{π}(x^{(t)}∣x^{(t−1)};β_t) \\ (2)\]

<p>즉, 노이즈를 점점 추가하는 Markov chain입니다.</p>
<ul>
  <li>$T_{π}$: 확산 커널 (Gaussian, Binomial 등)</li>
  <li>$β_{t}$: 확산 강도(시간마다 다르게 설정 가능)</li>
  <li>𝑇: 전체 타임스텝 수</li>
</ul>

<p>전체 확률 경로:</p>

\[q(x^{(0:T)})=q(x^{(0)}) \displaystyle\prod^{t=1}_{t=1}q(x^{(t)}∣x^{(t−1)}) \\ (5)\]

<hr />

<h3 id="22-reverse-trajectory-generative-process">2.2 Reverse Trajectory (Generative Process)</h3>
<h4 id="221-핵심-개념">2.2.1 핵심 개념</h4>
<ul>
  <li>정규분포 $𝜋(𝑥^{(𝑇)})$ 에서 시작해서, 학습된 reverse Markov chain을 거쳐 원래 데이터 분포로 되돌아가는 생성 모델.</li>
</ul>

\[p(x^{(0:T)})=p(x^{(T)}) \displaystyle\prod_{t=1}^{T} p(x^{(t−1)} ∣x ^{(t)})\]

<h5 id="핵심">핵심</h5>

<ul>
  <li>
    <p>각 스텝 $p(x^{(t−1)}|x^{(t)})$ 는 forward의 역과 같은 구조를 갖되, mean, covariance 또는 flip 확률만 학습하면 됩니다.</p>
  </li>
  <li>
    <p>Gaussian: mean $f_{μ}(x^{(t)} ,t)$, covariance $f_Σ (x^{(t)} ,t)$</p>
  </li>
  <li>
    <p>Binomial: bit flip rate $f_b(x^{(t)},t)$</p>
  </li>
</ul>

<hr />

<h3 id="23-model-probability-log-likelihood">2.3 Model Probability (Log-Likelihood)</h3>

<p>모델 데이터의 분포는 적분식(6)이므로 tractable 하지 않아, 순방향 경로에 대한 sampling을 평균화 하여 효율적으로 사용</p>

<p>직접 $p(x^{(0)})$ 를 계산하기는 어렵지만, forward/reverse 경로의 확률비를 계산해 근사합니다:</p>

\[p(x^{(0)})=\int dx^{(1...T)}q\left(x^{(1...T)}|x^{(0)}\right) \cdot p(x^{(T)}) \prod_{t=1}^{T}\frac{p(x^{(t-1)}|x^t)}{q(x^{(t)}|x^{(t-1)})} \\ (9)\]

\[p(x^{(0)})=E_{q(x^{(1:T)}∣x^{(0)})}\left[\frac{p(x^{(0:T)})}{q(x^{(1:T)}|x^{(0)})} \right]\]

<p>이건 Annealed Importance Sampling과 Jarzynski Equality와 유사한 방식입니다.</p>

<h4 id="개념정리-log-likelihood란">개념정리 Log-Likelihood란?</h4>
<p>Likelihood</p>
<ul>
  <li>어떤 데이터 $x$가 관측되었을때, 모델이 그 데이터를 낼 확률</li>
  <li>$p_{\theta}(x)$:파라미터 $\theta$를 가진 모델이 $x$를 생성할 확률</li>
  <li>
    <p>여러 데이터가 있을 경우 전체 likelihood는 곱</p>

\[L(\theta)=\displaystyle\prod_{i=1}^Np_{\theta}(x_i)\]
  </li>
  <li>Log-Likelihood는 위 확률의 로그를 취한 것</li>
  <li>곱이 너무 작아지는 것을 방지, 수학적으로 미분이 쉬워서 최적화에 유리</li>
</ul>

<hr />

<h3 id="24-training-log-likelihood-bound-maximization">2.4 Training (Log-likelihood Bound Maximization)</h3>
<p>로그 가능도(Log likelihood): 
\(L=\int dx^{(0)}q\left(x^{(0)}\right) \log p \left( x^{(0)} \right) \\ (10)\)
\(𝐿=𝐸_{𝑞(𝑥^{(0)})}[\log ⁡𝑝(𝑥^{(0)})]\)</p>

<p>여기에 (9)번식 대입하여 Jensen’s inequality로 lower bound 𝐾를 도입:</p>

\[= \int dx^{(0)}q\left(x^{(0)}\right) \log p(x^{(0)}) \\ (10)\]

\[𝐿≥𝐾=−\displaystyle\sum_{𝑡=2}^{𝑇} \int dx^{(0)}dx^{(t)}q(x^{(0)},x^{(t)}) \cdot [𝐷_{𝐾𝐿}(𝑞(𝑥^{(𝑡−1)}∣𝑥^{(𝑡)},𝑥^{(0)}) \parallel 𝑝(𝑥^{(𝑡−1)}∣𝑥^{(𝑡)}))] + H_q(X^{(T)}|X^{(0)}) - H_q(X^{(1)}|X^{(0)}) - H_p(X^{(T)}) \\ (14)\]

\[𝐿≥𝐾=−\displaystyle\sum_{𝑡=2}^{𝑇}𝐸_{𝑞(𝑥^{(0)},𝑥^{(𝑡)})}[𝐷_{𝐾𝐿}(𝑞(𝑥^{(𝑡−1)}∣𝑥^{(𝑡)},𝑥^{(0)})∥𝑝(𝑥^{(𝑡−1)}∣𝑥^{(𝑡)}))]+entropy terms\]

<p>즉, reverse transition과 posterior 간의 KL divergence를 최소화하는 것이 학습의 핵심입니다.</p>

<ul>
  <li>$D_{KL}$ (Kullback-Leibler Divergence)</li>
  <li>$D_{KL}(P∣∣Q)=∑_x P(x) \log \frac{P(x)}{Q(x)}$</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">제목</th>
      <th style="text-align: left">내용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">$q(x^{(t)}∣x^{(0)})$</td>
      <td style="text-align: left">forward diffusion 과정: $x^{(0)}→x^{(T)}$</td>
    </tr>
    <tr>
      <td style="text-align: left">$p(x^{(t−1)}∣x^{(t)})$</td>
      <td style="text-align: left">학습하고자 하는 reverse process (생성 모델)</td>
    </tr>
    <tr>
      <td style="text-align: left">$D_{KL}(q∣∣p) $</td>
      <td style="text-align: left">두 분포 사이의 차이 측정</td>
    </tr>
    <tr>
      <td style="text-align: left">$E_{q(x^{(0)},x^{(t)})}[⋅]$</td>
      <td style="text-align: left">forward process에서 샘플링된 샘플로부터 기대값을 계산</td>
    </tr>
    <tr>
      <td style="text-align: left">entropy terms</td>
      <td style="text-align: left">일부는 정규화상수 또는 모델에 상관없는 항으로 무시 가능</td>
    </tr>
  </tbody>
</table>

<p>✅ 학습 대상: 각 스텝의 reverse kernel (mean, covariance, flip rate 등)</p>

<h4 id="241-diffusion-schedule-𝛽_𝑡">2.4.1 Diffusion Schedule $𝛽_𝑡$</h4>
<ul>
  <li>
    <p>Gaussian: $𝛽_1$은 고정, 나머지는 gradient로 학습 가능</p>
  </li>
  <li>
    <p>Binomial: $𝛽_𝑡 = \frac{1}{𝑇−𝑡+1}$등 일정한 노이즈 감소 스케줄 사용</p>
  </li>
</ul>

<hr />

<h3 id="25-posterior-계산-및-분포-곱셈">2.5 Posterior 계산 및 분포 곱셈</h3>

<p>문제 : 이미지 복원, inpainting, denoising 등에서는 $p(x^{(0)}∣known data)$ 계산이 필요합니다.</p>

<p>해결: Diffusion model은 임의의 함수 $r(x^{(0)})$를 곱해서 새로운 분포 $\tilde{p}(x^{(0)})∝p(x^{(0)})r(x{(0)})$를 구성할 수 있습니다.</p>

<p>이는 reverse kernel에 perturbation을 주는 방식으로 처리됨</p>

<p>Gaussian인 경우엔 closed-form으로 처리 가능</p>

<hr />

<h3 id="posterior란-사후확률">Posterior란? (사후확률)</h3>

<h4 id="정의">정의</h4>

<p>어떤 관측값 y가 주어졌을 때, 잠재 변수 𝑥가 실제로 어떤 값일지를 예측하는 확률분포예요.</p>

\[p(x|y) = \frac{p(x) \cdot p(y|x)}{p(y)}\]

<ul>
  <li>
    <p>$p(x)$: 사전 확률 (prior)</p>
  </li>
  <li>
    <p>$p(y∣x)$: likelihood (관측값의 조건부 확률)</p>
  </li>
  <li>
    <p>$p(x∣y)$: posterior (우리가 알고 싶은 것)</p>
  </li>
  <li>
    <p>$p(y)$: 정규화 상수</p>
  </li>
  <li>
    <p>“이런 결과가 관측되었는데, 원인은 무엇일까?” →  원인을 추정하는 역추론적 사고가 posterior입니다.</p>
  </li>
</ul>

<h4 id="논문-맥락에서의-posterior">논문 맥락에서의 Posterior</h4>

\[posterior \propto p(x^{(0)}) \cdot r(x^{(0)})\]

<ul>
  <li>$p(x^{(0)})$ : 모델 분포</li>
  <li>$r(x^{(0)})$ : 조건, 관측된 부분분</li>
</ul>

<hr />

<h3 id="appendix">Appendix</h3>
<h4 id="1-monte-carlo">1. Monte Carlo</h4>
<ul>
  <li>정확한 계산이 어렵거나 불가능할 때, 많은 수의 랜덤 샘플을 뽑아서 평균을 내면 근사값이 된다는 아이디어입니다.</li>
</ul>

<h5 id="1-1-예시">1-1. 예시</h5>
<ul>
  <li>정적분 근사 어떤 함수 $f(x)$의 정적분을 계산하고 싶을 때, $I = \int\limits_a^b f(x)dx$ 이걸 직접 계산하기 어렵다면,</li>
  <li>구간 [𝑎, 𝑏]에서 무작위로 N개의 샘플 $X_1, … X_n$을 뽑아서</li>
</ul>

<p>$I \approx \frac{𝑏−𝑎}{N} \displaystyle\sum_{i=1}^{N} f(x_i) $
처럼 샘플 평균 근사 가능</p>

<h5 id="1-2-단점">1-2. 단점</h5>
<ul>
  <li>계산량이 많아요. (샘플 수가 커야 정확해짐)</li>
  <li>분산이 클 수 있음 → 샘플 수가 부족하면 근사값이 매우 부정확</li>
  <li>좋은 샘플링 분포를 잘 선택해야 함 (안 그러면 “희소 영역”은 놓침)</li>
</ul>

<hr />

<h4 id="2-variational-bayes-vb">2. Variational Bayes (VB)</h4>
<h5 id="2-1-목적">2-1. 목적</h5>

<p>복잡한 <strong>posterior $p(z∣x)$</strong>를 직접 계산하기 어려울 때, tractable한 분포 $q(z)$ 로 근사해서 <strong>Evidence Lower Bound (ELBO)</strong>를 최적화함.</p>

<h5 id="2-2-아이디어">2-2. 아이디어</h5>
<ul>
  <li>$\log p(x) ≥ E_{q(z)} [\log p(x,z) − \log q(z)]$</li>
</ul>

<h5 id="2-3-한계">2-3. 한계:</h5>
<ul>
  <li>선택한 $q(z)$가 실제 posterior를 잘 못 따라가면 부정확</li>
  <li>모델과 inference 분포 사이 비대칭성 → 학습 어려움</li>
</ul>

<h4 id="3-contrastive-divergence-cd">3. Contrastive Divergence (CD)</h4>
<h5 id="3-1-목적">3-1. 목적</h5>
<ul>
  <li>Boltzmann machine 같이 정규화 상수 Z가 없는 에너지 기반 모델의 파라미터 학습</li>
</ul>

<h5 id="3-2-아이디어">3-2. 아이디어:</h5>
<ul>
  <li>Gibbs Sampling으로 한두 step만 진행하여 실제 분포와 모델 분포 간 차이를 줄임</li>
  <li>학습 대상: 데이터 분포와 모델 분포 간의 차이 (score function)</li>
</ul>

<p>$Δθ∝E_{data}[f(x)]−E_{model}[f(x)]$</p>

<h5 id="3-3-한계">3-3. 한계:</h5>
<ul>
  <li>근사 샘플링이기 때문에 이론적으로 보장 안 됨</li>
  <li>많게는 수천 스텝 필요 → 비효율적</li>
</ul>

<h4 id="4-score-matching">4. Score Matching</h4>
<h5 id="4-1-목적">4-1. 목적:</h5>
<ul>
  <li>정규화 상수 없는 확률분포에서도 학습 가능하게 함</li>
</ul>

<h5 id="4-2-아이디어">4-2. 아이디어:</h5>
<p>score function $∇_x \log p(x)$을 이용해 다음을 최소화:</p>

\[J(θ)=E_{p_{data}} [∥∇_{x} \log p_{θ}(x)−∇_{x} \log p_{data}(x)∥^{2}]\]

<h5 id="4-3-한계">4-3. 한계:</h5>
<ul>
  <li>2차 도함수 계산 필요 → 고차원에서는 느림</li>
  <li>샘플링 자체는 불가능</li>
</ul>

<h4 id="5-pseudolikelihood">5. Pseudolikelihood</h4>
<h5 id="5-1-목적">5-1. 목적</h5>
<ul>
  <li>Markov Random Field 같은 모델에서 복잡한 joint likelihood 대신 조건부 확률만 사용</li>
</ul>

\[PL(x)= \displaystyle\prod_{i}p(x_i|x_{i-1})\]

<h5 id="5-2-한계">5-2. 한계</h5>
<ul>
  <li>조건부 확률만 최대화 → global 구조 학습에는 한계</li>
  <li>Likelihood를 직접 최적화하는 것보다 정확도 낮을 수 있음</li>
</ul>

<h4 id="6-loopy-belief-propagation-lbp">6. Loopy Belief Propagation (LBP)</h4>

<h5 id="6-1-목적">6-1. 목적</h5>
<ul>
  <li>그래프 모델 (특히 MRF, CRF)에서 근사적인 marginal inference</li>
</ul>

<h5 id="6-2-아이디어">6-2. 아이디어</h5>
<ul>
  <li>
    <p>메시지 전달 알고리즘을 사이클이 있는 그래프에도 적용</p>
  </li>
  <li>
    <p>반복적 메시지 전달을 통해 근사분포 계산</p>
  </li>
</ul>

<h5 id="6-3-한계">6-3. 한계</h5>
<ul>
  <li>
    <p>수렴이 보장되지 않음</p>
  </li>
  <li>
    <p>근사 정확도 떨어질 수 있음</p>
  </li>
</ul>

<h4 id="7-mean-field-theory">7. Mean Field Theory</h4>
<h5 id="7-1-목적">7-1. 목적</h5>
<ul>
  <li>복잡한 분포를 독립된 단일 변수의 곱으로 근사</li>
</ul>

<p>$p(x)≈ \displaystyle\prod_{i} q_{i}(x_{i})$</p>

<h5 id="7-2-한계">7-2. 한계</h5>
<p>변수 간 의존성 무시 → 복잡한 구조 표현 불가능</p>

<p>간단한 구조에서는 작동하지만 일반화에 약함</p>

<h4 id="8-minimum-probability-flow-mpf">8. Minimum Probability Flow (MPF)</h4>
<h5 id="8-1-목적">8-1. 목적</h5>
<ul>
  <li>Energy-based model에서 정규화 상수 없이도 학습 가능하게 함</li>
</ul>

<h5 id="8-2-아이디어">8-2. 아이디어</h5>
<ul>
  <li>데이터 분포 $p_{data}$에서 가까운 이웃 상태로의 확률 흐름을 줄이도록 학습</li>
  <li>즉, 데이터와 비데이터 사이의 flow를 줄이기</li>
</ul>

<p>$min_{𝜃} \displaystyle\sum_{𝑥∈data} \displaystyle\sum_{𝑥’}𝑇(𝑥′∣𝑥) \log \frac{𝑝_{𝜃}(𝑥′)}{𝑝_{𝜃}(𝑥)}$</p>

<h5 id="8-3-한계">8-3. 한계</h5>
<ul>
  <li>이웃 상태 정의에 따라 결과가 민감</li>
  <li>복잡한 분포에는 한계</li>
</ul>

<hr />

<h5 id="9-annealed-importance-sampling-ais">9. Annealed Importance Sampling (AIS)</h5>
<ul>
  <li>복잡한 분포에서 샘플링</li>
  <li>중간 분포열 설정: AIS는 쉬운 시작 분포 $(π_0)$와 샘플링하고자 하는 목표 분포 $(π_N)$ 사이에 일련의 중간 분포들 $(π_1,π_2,…,π_{N−1})$ 을 정의</li>
  <li>분포가 부드럽게 전환되도록 설계, $π_k(x)∝(π_0(x))^{1−β_k}(π_N(x))^{β_k}$와 같은 형태로 정의할 수 있으며, $β_k$는 0에서 1까지 점진적으로 증가</li>
  <li>중요도 가중치 경로 $(x_0, \ldots, x_N)$에 대한 가중치 w는 다음과 같이 정의됩니다.
\(w=\frac{π_1(x_0)π_2(x_1)}{π_0(x_0)π_1(x_1)} ⋯ \frac{π_N(x_{N−1})}{π_{N−1}(x_{N−1})} \frac{전이확률 역과정}{전이확률 정과정}\)  (정확한 형태는 사용된 MCMC 전이 커널에 따라 달라진다)</li>
</ul>

<h5 id="10-jarzynski-equality">10. Jarzynski Equality</h5>
<ul>
  <li>
    <p>Jarzynski Equality는 비평형 과정을 통해 두 평형 상태 간의 자유 에너지 차이(ΔF)를 통계적으로 연결하는 중요한 등식</p>
  </li>
  <li>
    <p>수식: $ΔF=−k_BTln⟨e^{−W/k_BT}⟩$</p>
    <ul>
      <li>$ΔF$: 초기 평형 상태와 최종 평형 상태 사이의 헬름홀츠 자유 에너지 변화</li>
      <li>$k_B$ : 볼츠만 상수</li>
      <li>$T$: 온도</li>
      <li>$W$: 비평형 과정 동안 시스템에 가해진 일 (work)</li>
      <li>$⟨⋅⟩$: 비평형 과정의 앙상블 평균 (즉, 동일한 비평형 과정을 여러 번 반복했을 때 얻어지는 일 W 값들의 지수 평균)</li>
    </ul>
  </li>
</ul>

<p>Jarzynski Equality는 고전 열역학의 제2법칙(ΔF≤W)을 통계역학적으로 확장한 것으로 해석될 수 있습니다.
$\left&lt; e^{-W/k_B T} \right&gt;$ 는 항상 $e^{-\left&lt; W \right&gt;/k_B T}$보다 크거나 같으므로, $\Delta F \le \left&lt; W \right&gt;$가 성립합니다. 이는 평균적으로는 비가역 과정의 일이 항상 자유 에너지 변화보다 크거나 같다는 열역학 제2법칙을 만족합니다.</p>

      </div>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#test">test</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  

  

  
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=%2F2025-08-21-%25EB%2585%25BC%25EB%25AC%25B8%25EB%25A6%25AC%25EB%25B7%25B0-Deep-Unsupervised-Learning-using-Nonequilibrium-Thermodynamics-%282015-ICML%29%2F"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fab fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

  

  

</section>



      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2025-08-21-Flash-Attention/" data-toggle="tooltip" data-placement="top" title="Flash Attention">
            <i class="fas fa-arrow-left" alt="Previous Post"></i>
            <span class="d-none d-sm-inline-block">Previous Post</span>
          </a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2025-08-21-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Denoising-Diffusion-Implicit-Models(DDIM)/" data-toggle="tooltip" data-placement="top" title="[논문리뷰]Denoising Diffusion Implicit Models(DDIM)">
            <span class="d-none d-sm-inline-block">Next Post</span>
            <i class="fas fa-arrow-right" alt="Next Post"></i>
          </a>
        </li>
        
      </ul>
      
  
  
  

  


  

  



    </div>
  </div>
</main>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      
<ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:younghyeok25@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/https://github.com/james-hyeok-kim/james-hyeok-kim.github.io.git" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/https://www.linkedin.com/in/jameskim525" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li></ul>


      
      <p class="copyright text-muted">
      
        James kim
        &nbsp;&bull;&nbsp;
      
      2025

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="/">MyWebsite.com</a>
        </span>
      

      

      

      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
